{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopandas\n",
    "!pip install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒˆ ì…€ì—ì„œ ì‹¤í–‰\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "    print(\"âœ… GeoPandas ì¤€ë¹„ë¨\")\n",
    "except ImportError:\n",
    "    print(\"âŒ GeoPandas ì„¤ì¹˜ í•„ìš”\")\n",
    "    # conda install -c conda-forge geopandas\n",
    "\n",
    "try:\n",
    "    from shapely.geometry import Point\n",
    "    print(\"âœ… Shapely ì¤€ë¹„ë¨\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Shapely ì„¤ì¹˜ í•„ìš”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒˆ ì…€ì—ì„œ ì‹¤í–‰ - ê° ë°ì´í„°ì…‹ì˜ ì‹¤ì œ ì»¬ëŸ¼ëª… í™•ì¸\n",
    "print(\"ğŸ“‹ ê° ë°ì´í„°ì…‹ì˜ ìƒê¶Œ ê´€ë ¨ ì»¬ëŸ¼ í™•ì¸:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "datasets_check = {\n",
    "    'boundary': boundary_df,\n",
    "    'flow_latest': flow_latest, \n",
    "    'resident_latest': resident_latest,\n",
    "    'sales_latest': sales_latest,\n",
    "    'income_latest': income_latest,\n",
    "    'facility_latest': facility_latest\n",
    "}\n",
    "\n",
    "for name, df in datasets_check.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    # ìƒê¶Œ ê´€ë ¨ ì»¬ëŸ¼ ì°¾ê¸°\n",
    "    district_cols = [col for col in df.columns if any(keyword in col \n",
    "                    for keyword in ['ìƒê¶Œ', 'TRDAR', 'ì½”ë“œ', 'CD', 'CODE'])]\n",
    "    print(f\"  ìƒê¶Œ ê´€ë ¨ ì»¬ëŸ¼: {district_cols}\")\n",
    "    \n",
    "    # ì²« 5ê°œ ì»¬ëŸ¼ë§Œ í‘œì‹œ\n",
    "    print(f\"  ì „ì²´ ì»¬ëŸ¼ (ì• 5ê°œ): {list(df.columns)[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) í™•ì¸í•  ë³€ìˆ˜ëª… ë§¤í•‘ (í‚¤: ì—¬ëŸ¬ë¶„ì´ ë¶€ë¥´ê³  ì‹¶ì€ ì´ë¦„, ê°’: ì‹¤ì œ ë³€ìˆ˜ëª…)\n",
    "vars_to_check = {\n",
    "    'boundary': 'boundary_df',\n",
    "    'flow_latest': 'flow_latest',\n",
    "    'resident_latest': 'resident_latest',\n",
    "    'sales_latest': 'sales_latest',\n",
    "    'income_latest': 'income_latest',\n",
    "    'facility_latest': 'facility_latest'\n",
    "}\n",
    "\n",
    "# 2) ì¡´ì¬ ì—¬ë¶€, íƒ€ì…, shape, columns ì¶œë ¥\n",
    "for key, var_name in vars_to_check.items():\n",
    "    if var_name in globals():\n",
    "        df = globals()[var_name]\n",
    "        try:\n",
    "            rows, cols = df.shape\n",
    "            print(f\"âœ… {key} ({var_name}): {type(df).__name__}, shape={rows}Ã—{cols}\")\n",
    "            print(\"   columns:\", df.columns.tolist())\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ {key} ({var_name}): ë³€ìˆ˜ëŠ” ì¡´ì¬í•˜ì§€ë§Œ .shape/.columnsë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ ({e})\")\n",
    "    else:\n",
    "        print(f\"âŒ {key}: ë³€ìˆ˜ `{var_name}` ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒˆ ì…€ì—ì„œ ì‹¤í–‰ - ê°œì„ ëœ ìƒê¶Œì½”ë“œ í†µì¼ í•¨ìˆ˜\n",
    "def standardize_district_code_fixed(df, dataset_name):\n",
    "    \"\"\"ìƒê¶Œì½”ë“œ ì»¬ëŸ¼ì„ TRDAR_CDë¡œ í†µì¼ (ì‹¤ì œ ì»¬ëŸ¼ëª… ê¸°ë°˜)\"\"\"\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ë³„ ë§¤í•‘ ê·œì¹™\n",
    "    column_mapping = {\n",
    "        'boundary': 'ìƒê¶Œ_ì½”ë“œ',\n",
    "        'flow': 'ìƒê¶Œ_ì½”ë“œ', \n",
    "        'resident': 'market_code',      # â† ì´ê²Œ í•µì‹¬!\n",
    "        'sales': 'ìƒê¶Œ_ì½”ë“œ',\n",
    "        'income': 'ìƒê¶Œ_ì½”ë“œ',\n",
    "        'facility': 'ìƒê¶Œë°°í›„ì§€_ì½”ë“œ'    # â† ì´ê²ƒë„!\n",
    "    }\n",
    "    \n",
    "    target_col = column_mapping.get(dataset_name, 'ìƒê¶Œ_ì½”ë“œ')\n",
    "    \n",
    "    if target_col in df.columns:\n",
    "        df['TRDAR_CD'] = df[target_col]\n",
    "        print(f\"âœ… {dataset_name}: {target_col} â†’ TRDAR_CD\")\n",
    "        return True\n",
    "    else:\n",
    "        # ë°±ì—… ë°©ë²•ë“¤\n",
    "        possible_cols = ['TRDAR_CD', 'ìƒê¶Œ_ì½”ë“œ', 'market_code', 'ìƒê¶Œë°°í›„ì§€_ì½”ë“œ']\n",
    "        for col in possible_cols:\n",
    "            if col in df.columns:\n",
    "                if col != 'TRDAR_CD':\n",
    "                    df['TRDAR_CD'] = df[col]\n",
    "                print(f\"âœ… {dataset_name}: {col} â†’ TRDAR_CD (ë°±ì—…)\")\n",
    "                return True\n",
    "    \n",
    "    print(f\"âŒ {dataset_name}: ìƒê¶Œì½”ë“œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ\")\n",
    "    return False\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "print(\"ğŸ§ª ìƒê¶Œì½”ë“œ í†µì¼ í…ŒìŠ¤íŠ¸:\")\n",
    "standardize_district_code_fixed(resident_latest, 'resident')\n",
    "standardize_district_code_fixed(facility_latest, 'facility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ—ï¸ ê³±ì°½ì§‘ ì…ì§€ ë¶„ì„ ë§ˆìŠ¤í„° ë°ì´í„°ì…‹ êµ¬ì¶•\n",
    "# 02_master_build.ipynb\n",
    "# \n",
    "# ëª©ì : ëª¨ë“  ë°ì´í„°ë¥¼ TRDAR_CD ê¸°ì¤€ìœ¼ë¡œ í†µí•©í•˜ì—¬ \n",
    "#       seoul_gopchang_master.gpkg ë§ˆìŠ¤í„° ë°ì´í„°ì…‹ ìƒì„±\n",
    "\n",
    "# ================================================================================\n",
    "# ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "# ================================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ì„¤ì •\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"ğŸ—ï¸ ê³±ì°½ì§‘ ì…ì§€ ë¶„ì„ ë§ˆìŠ¤í„° ë°ì´í„°ì…‹ êµ¬ì¶• ì‹œì‘!\")\n",
    "print(f\"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ================================================================================\n",
    "# ğŸ“‚ ëª¨ë“  ë°ì´í„° ë¡œë“œ\n",
    "# ================================================================================\n",
    "\n",
    "print(\"ğŸ“‚ ëª¨ë“  ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# ë°ì´í„° ê²½ë¡œ ì„¤ì •\n",
    "DATA_PATH = Path(\"../data\")\n",
    "RAW_PATH = DATA_PATH / \"raw\"\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ ìƒíƒœ ì¶”ì \n",
    "data_status = {}\n",
    "\n",
    "try:\n",
    "    # 1. ìƒê¶Œ ê²½ê³„ ë°ì´í„° (GIS ê¸°ì¤€ì )\n",
    "    print(\"1ï¸âƒ£ ìƒê¶Œ ê²½ê³„ ë°ì´í„° ë¡œë“œ...\")\n",
    "    boundary_df = pd.read_csv(DATA_PATH / \"boundary/boundary_all.csv\", encoding='utf-8')\n",
    "    data_status['boundary'] = f\"âœ… ì„±ê³µ ({boundary_df.shape})\"\n",
    "    print(f\"   ğŸ“ ìƒê¶Œ ê²½ê³„: {boundary_df.shape}\")\n",
    "    \n",
    "    # 2. ìœ ë™ì¸êµ¬ ë°ì´í„° (ì•¼ê°„ ë¶„ì„ ì™„ë£Œ)\n",
    "    print(\"2ï¸âƒ£ ìœ ë™ì¸êµ¬ ë°ì´í„° ë¡œë“œ...\")\n",
    "    flow_df = pd.read_csv(RAW_PATH / \"flow/flow_all.csv\", encoding='utf-8')\n",
    "    data_status['flow'] = f\"âœ… ì„±ê³µ ({flow_df.shape})\"\n",
    "    print(f\"   ğŸ‘¥ ìœ ë™ì¸êµ¬: {flow_df.shape}\")\n",
    "    \n",
    "    # 3. ìƒì£¼ì¸êµ¬ ë°ì´í„°\n",
    "    print(\"3ï¸âƒ£ ìƒì£¼ì¸êµ¬ ë°ì´í„° ë¡œë“œ...\")\n",
    "    resident_df = pd.read_csv(RAW_PATH / \"resident/resident_all.csv\", encoding='utf-8')\n",
    "    data_status['resident'] = f\"âœ… ì„±ê³µ ({resident_df.shape})\"\n",
    "    print(f\"   ğŸ  ìƒì£¼ì¸êµ¬: {resident_df.shape}\")\n",
    "    \n",
    "    # 4. ì¶”ì •ë§¤ì¶œ ë°ì´í„°\n",
    "    print(\"4ï¸âƒ£ ì¶”ì •ë§¤ì¶œ ë°ì´í„° ë¡œë“œ...\")\n",
    "    sales_df = pd.read_csv(RAW_PATH / \"sales/sales_all.csv\", encoding='utf-8')\n",
    "    data_status['sales'] = f\"âœ… ì„±ê³µ ({sales_df.shape})\"\n",
    "    print(f\"   ğŸ’° ì¶”ì •ë§¤ì¶œ: {sales_df.shape}\")\n",
    "    \n",
    "    # 5. ì†Œë“Â·ì†Œë¹„ ë°ì´í„°\n",
    "    print(\"5ï¸âƒ£ ì†Œë“Â·ì†Œë¹„ ë°ì´í„° ë¡œë“œ...\")\n",
    "    income_df = pd.read_csv(RAW_PATH / \"income_exp/income_all.csv\", encoding='utf-8')\n",
    "    data_status['income'] = f\"âœ… ì„±ê³µ ({income_df.shape})\"\n",
    "    print(f\"   ğŸ’³ ì†Œë“Â·ì†Œë¹„: {income_df.shape}\")\n",
    "    \n",
    "    # 6. ì§‘ê°ì‹œì„¤ ë°ì´í„°\n",
    "    print(\"6ï¸âƒ£ ì§‘ê°ì‹œì„¤ ë°ì´í„° ë¡œë“œ...\")\n",
    "    facility_df = pd.read_csv(RAW_PATH / \"facility/facility_all.csv\", encoding='utf-8')\n",
    "    data_status['facility'] = f\"âœ… ì„±ê³µ ({facility_df.shape})\"\n",
    "    print(f\"   ğŸ¢ ì§‘ê°ì‹œì„¤: {facility_df.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}\")\n",
    "    data_status['error'] = str(e)\n",
    "\n",
    "print(f\"\\nğŸ“Š ì´ {len([k for k, v in data_status.items() if 'âœ…' in v])}ê°œ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# ================================================================================\n",
    "# ğŸ” ë°ì´í„° êµ¬ì¡° ë° í’ˆì§ˆ ë¶„ì„\n",
    "# ================================================================================\n",
    "\n",
    "print(\"ğŸ” ë°ì´í„° êµ¬ì¡° ë° í’ˆì§ˆ ë¶„ì„\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# ê° ë°ì´í„°ì…‹ë³„ TRDAR_CD (ìƒê¶Œì½”ë“œ) ë¶„ì„\n",
    "datasets = {\n",
    "    'boundary': boundary_df,\n",
    "    'flow': flow_df, \n",
    "    'resident': resident_df,\n",
    "    'sales': sales_df,\n",
    "    'income': income_df,\n",
    "    'facility': facility_df\n",
    "}\n",
    "\n",
    "trdar_analysis = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    if 'TRDAR_CD' in df.columns or 'ìƒê¶Œ_ì½”ë“œ' in df.columns:\n",
    "        # ìƒê¶Œì½”ë“œ ì»¬ëŸ¼ ì°¾ê¸°\n",
    "        trdar_col = 'TRDAR_CD' if 'TRDAR_CD' in df.columns else 'ìƒê¶Œ_ì½”ë“œ'\n",
    "        \n",
    "        unique_codes = df[trdar_col].nunique()\n",
    "        total_rows = len(df)\n",
    "        latest_quarter = df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].max() if 'ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ' in df.columns else 'N/A'\n",
    "        \n",
    "        trdar_analysis[name] = {\n",
    "            'unique_districts': unique_codes,\n",
    "            'total_rows': total_rows,\n",
    "            'latest_quarter': latest_quarter,\n",
    "            'trdar_column': trdar_col\n",
    "        }\n",
    "        \n",
    "        print(f\"ğŸ“‹ {name:10s}: {unique_codes:4d}ê°œ ìƒê¶Œ, {total_rows:6d}í–‰, ìµœì‹ ë¶„ê¸°: {latest_quarter}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ê¸°ì¤€ ìƒê¶Œ ìˆ˜ (boundary): {trdar_analysis['boundary']['unique_districts']}ê°œ\")\n",
    "\n",
    "# ê¸°ì¤€ë…„ë¶„ê¸° í†µì¼ ë¶„ì„\n",
    "print(f\"\\nğŸ“… ê¸°ì¤€ë…„ë¶„ê¸° ë¶„ì„:\")\n",
    "for name, df in datasets.items():\n",
    "    if 'ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ' in df.columns:\n",
    "        quarters = sorted(df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].unique(), reverse=True)\n",
    "        print(f\"   {name:10s}: {quarters[:3]}... (ì´ {len(quarters)}ê°œ ë¶„ê¸°)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# ================================================================================\n",
    "# âš™ï¸ ë°ì´í„° ì „ì²˜ë¦¬ ë° í†µí•© ì¤€ë¹„\n",
    "# ================================================================================\n",
    "\n",
    "print(\"âš™ï¸ ë°ì´í„° ì „ì²˜ë¦¬ ë° í†µí•© ì¤€ë¹„\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 1. ìµœì‹  ë¶„ê¸° ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜\n",
    "def get_latest_quarter_data(df, name):\n",
    "    \"\"\"ìµœì‹  ë¶„ê¸° ë°ì´í„°ë§Œ ì¶”ì¶œ\"\"\"\n",
    "    if 'ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ' not in df.columns:\n",
    "        print(f\"   {name}: ê¸°ì¤€ë…„ë¶„ê¸° ì—†ìŒ (ì „ì²´ ë°ì´í„° ì‚¬ìš©)\")\n",
    "        return df\n",
    "    \n",
    "    latest_quarter = df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].max()\n",
    "    latest_df = df[df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] == latest_quarter].copy()\n",
    "    \n",
    "    print(f\"   {name}: {latest_quarter} â†’ {len(latest_df):,}í–‰\")\n",
    "    return latest_df\n",
    "\n",
    "# 2. ìµœì‹  ë¶„ê¸° ë°ì´í„° ì¶”ì¶œ\n",
    "print(\"ğŸ“… ìµœì‹  ë¶„ê¸° ë°ì´í„° ì¶”ì¶œ:\")\n",
    "\n",
    "flow_latest = get_latest_quarter_data(flow_df, 'flow')\n",
    "resident_latest = get_latest_quarter_data(resident_df, 'resident') \n",
    "sales_latest = get_latest_quarter_data(sales_df, 'sales')\n",
    "income_latest = get_latest_quarter_data(income_df, 'income')\n",
    "facility_latest = get_latest_quarter_data(facility_df, 'facility')\n",
    "\n",
    "# 3. ì•¼ê°„ ìœ ë™ì¸êµ¬ ê³„ì‚° (ì´ì „ ë¶„ì„ ê²°ê³¼ ì ìš©)\n",
    "print(f\"\\nğŸŒ™ ì•¼ê°„ ìœ ë™ì¸êµ¬ ê³„ì‚° (17-24ì‹œ ê¸°ì¤€):\")\n",
    "\n",
    "def calculate_night_flow(df):\n",
    "    \"\"\"17-24ì‹œ ì•¼ê°„ ìœ ë™ì¸êµ¬ ê³„ì‚°\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 17-24ì‹œ = 17-21ì‹œ + 21-24ì‹œ\n",
    "    df['ì•¼ê°„_ìœ ë™ì¸êµ¬_17_24'] = (df['ì‹œê°„ëŒ€_17_21_ìœ ë™ì¸êµ¬_ìˆ˜'] + \n",
    "                               df['ì‹œê°„ëŒ€_21_24_ìœ ë™ì¸êµ¬_ìˆ˜'])\n",
    "    \n",
    "    # ì¶”ê°€ ì•¼ê°„ ì§€í‘œë“¤\n",
    "    df['ì•¼ê°„_ë¹„ìœ¨'] = df['ì•¼ê°„_ìœ ë™ì¸êµ¬_17_24'] / df['ì´_ìœ ë™ì¸êµ¬_ìˆ˜']\n",
    "    df['ì£¼ë§_í‰ê· '] = (df['í† ìš”ì¼_ìœ ë™ì¸êµ¬_ìˆ˜'] + df['ì¼ìš”ì¼_ìœ ë™ì¸êµ¬_ìˆ˜']) / 2\n",
    "    df['ì£¼ì¤‘_í‰ê· '] = (df['ì›”ìš”ì¼_ìœ ë™ì¸êµ¬_ìˆ˜'] + df['í™”ìš”ì¼_ìœ ë™ì¸êµ¬_ìˆ˜'] + \n",
    "                     df['ìˆ˜ìš”ì¼_ìœ ë™ì¸êµ¬_ìˆ˜'] + df['ëª©ìš”ì¼_ìœ ë™ì¸êµ¬_ìˆ˜'] + \n",
    "                     df['ê¸ˆìš”ì¼_ìœ ë™ì¸êµ¬_ìˆ˜']) / 5\n",
    "    df['ì£¼ë§_í”„ë¦¬ë¯¸ì—„'] = df['ì£¼ë§_í‰ê· '] / df['ì£¼ì¤‘_í‰ê· ']\n",
    "    \n",
    "    return df\n",
    "\n",
    "flow_latest = calculate_night_flow(flow_latest)\n",
    "print(f\"   âœ… ì•¼ê°„ ìœ ë™ì¸êµ¬ ê³„ì‚° ì™„ë£Œ\")\n",
    "print(f\"   ğŸ“Š í‰ê·  ì•¼ê°„ ìœ ë™ì¸êµ¬: {flow_latest['ì•¼ê°„_ìœ ë™ì¸êµ¬_17_24'].mean():,.0f}ëª…\")\n",
    "print(f\"   ğŸ“Š í‰ê·  ì•¼ê°„ ë¹„ìœ¨: {flow_latest['ì•¼ê°„_ë¹„ìœ¨'].mean():.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# ================================================================================\n",
    "# ğŸ”— ë§ˆìŠ¤í„° ë°ì´í„°ì…‹ êµ¬ì¶• (ë‹¨ê³„ë³„ ì¡°ì¸)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"ğŸ”— ë§ˆìŠ¤í„° ë°ì´í„°ì…‹ êµ¬ì¶• (ë‹¨ê³„ë³„ ì¡°ì¸)\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 1. ê¸°ì¤€ ë°ì´í„°: ìƒê¶Œ ê²½ê³„ (boundary)\n",
    "print(\"1ï¸âƒ£ ê¸°ì¤€ ë°ì´í„° ì„¤ì •: ìƒê¶Œ ê²½ê³„\")\n",
    "master_df = boundary_df.copy()\n",
    "\n",
    "# ìƒê¶Œì½”ë“œ ì»¬ëŸ¼ í†µì¼\n",
    "if 'ìƒê¶Œ_ì½”ë“œ' in master_df.columns:\n",
    "    master_df['TRDAR_CD'] = master_df['ìƒê¶Œ_ì½”ë“œ']\n",
    "\n",
    "print(f\"   ğŸ“ ê¸°ì¤€ ìƒê¶Œ ìˆ˜: {len(master_df):,}ê°œ\")\n",
    "\n",
    "# 2. ìœ ë™ì¸êµ¬ ë°ì´í„° ì¡°ì¸\n",
    "print(f\"\\n2ï¸âƒ£ ìœ ë™ì¸êµ¬ ë°ì´í„° ì¡°ì¸:\")\n",
    "before_count = len(master_df)\n",
    "\n",
    "# ìƒê¶Œì½”ë“œ ì»¬ëŸ¼ í†µì¼ í•¨ìˆ˜\n",
    "def standardize_district_code(df, dataset_name):\n",
    "    \"\"\"ìƒê¶Œì½”ë“œ ì»¬ëŸ¼ì„ TRDAR_CDë¡œ í†µì¼ (ë°ì´í„°ì…‹ë³„ ë§¤í•‘)\"\"\"\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ë³„ ì‹¤ì œ ì»¬ëŸ¼ëª… ë§¤í•‘\n",
    "    column_mapping = {\n",
    "        'boundary': 'ìƒê¶Œ_ì½”ë“œ',\n",
    "        'flow': 'ìƒê¶Œ_ì½”ë“œ', \n",
    "        'resident': 'market_code',      # residentëŠ” market_code ì‚¬ìš©\n",
    "        'sales': 'ìƒê¶Œ_ì½”ë“œ',\n",
    "        'income': 'ìƒê¶Œ_ì½”ë“œ',\n",
    "        'facility': 'ìƒê¶Œë°°í›„ì§€_ì½”ë“œ'    # facilityëŠ” ìƒê¶Œë°°í›„ì§€_ì½”ë“œ ì‚¬ìš©\n",
    "    }\n",
    "    \n",
    "    # ìš°ì„ ìˆœìœ„: ë§¤í•‘ëœ ì»¬ëŸ¼ â†’ ê¸°ë³¸ í›„ë³´ë“¤\n",
    "    target_col = column_mapping.get(dataset_name, 'ìƒê¶Œ_ì½”ë“œ')\n",
    "    fallback_cols = ['TRDAR_CD', 'ìƒê¶Œ_ì½”ë“œ', 'market_code', 'ìƒê¶Œë°°í›„ì§€_ì½”ë“œ', 'DISTRICT_CD']\n",
    "    \n",
    "    # 1ì°¨: ë§¤í•‘ëœ ì»¬ëŸ¼ ì‹œë„\n",
    "    if target_col in df.columns:\n",
    "        if target_col != 'TRDAR_CD':\n",
    "            df['TRDAR_CD'] = df[target_col]\n",
    "        print(f\"   ğŸ“‹ {dataset_name} ìƒê¶Œì½”ë“œ: {target_col} â†’ TRDAR_CD\")\n",
    "        return True\n",
    "    \n",
    "    # 2ì°¨: ë°±ì—… ì»¬ëŸ¼ë“¤ ì‹œë„\n",
    "    for col in fallback_cols:\n",
    "        if col in df.columns:\n",
    "            if col != 'TRDAR_CD':\n",
    "                df['TRDAR_CD'] = df[col]\n",
    "            print(f\"   ğŸ“‹ {dataset_name} ìƒê¶Œì½”ë“œ: {col} â†’ TRDAR_CD (ë°±ì—…)\")\n",
    "            return True\n",
    "    \n",
    "    print(f\"   âŒ {dataset_name}: ìƒê¶Œì½”ë“œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ\")\n",
    "    print(f\"   ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)[:10]}\")\n",
    "    return False\n",
    "\n",
    "# ìœ ë™ì¸êµ¬ ë°ì´í„° ìƒê¶Œì½”ë“œ í†µì¼\n",
    "if standardize_district_code(flow_latest, 'flow'):\n",
    "    # í•µì‹¬ ìœ ë™ì¸êµ¬ ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "    flow_cols = ['TRDAR_CD', 'ì´_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì•¼ê°„_ìœ ë™ì¸êµ¬_17_24', 'ì•¼ê°„_ë¹„ìœ¨',\n",
    "                 'ì£¼ë§_í‰ê· ', 'ì£¼ì¤‘_í‰ê· ', 'ì£¼ë§_í”„ë¦¬ë¯¸ì—„',\n",
    "                 'ë‚¨ì„±_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì—¬ì„±_ìœ ë™ì¸êµ¬_ìˆ˜',\n",
    "                 'ì—°ë ¹ëŒ€_20_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì—°ë ¹ëŒ€_30_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì—°ë ¹ëŒ€_40_ìœ ë™ì¸êµ¬_ìˆ˜']\n",
    "    \n",
    "    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "    available_flow_cols = [col for col in flow_cols if col in flow_latest.columns]\n",
    "    flow_selected = flow_latest[available_flow_cols].copy()\n",
    "    \n",
    "    master_df = master_df.merge(flow_selected, on='TRDAR_CD', how='left')\n",
    "    after_count = len(master_df)\n",
    "    \n",
    "    print(f\"   ğŸ“Š ì¡°ì¸ ì „: {before_count:,}ê°œ â†’ ì¡°ì¸ í›„: {after_count:,}ê°œ\")\n",
    "    print(f\"   ğŸ“ˆ ìœ ë™ì¸êµ¬ ë§¤ì¹­ë¥ : {(master_df['ì´_ìœ ë™ì¸êµ¬_ìˆ˜'].notna().sum() / len(master_df)):.1%}\")\n",
    "else:\n",
    "    print(f\"   âš ï¸ ìœ ë™ì¸êµ¬ ë°ì´í„° ì¡°ì¸ ê±´ë„ˆëœ€\")\n",
    "\n",
    "# 3. ìƒì£¼ì¸êµ¬ ë°ì´í„° ì¡°ì¸\n",
    "print(f\"\\n3ï¸âƒ£ ìƒì£¼ì¸êµ¬ ë°ì´í„° ì¡°ì¸:\")\n",
    "\n",
    "if standardize_district_code(resident_latest, 'resident'):\n",
    "    # ìƒì£¼ì¸êµ¬ ê´€ë ¨ ì»¬ëŸ¼ ì°¾ê¸°\n",
    "    resident_pop_cols = [col for col in resident_latest.columns if 'ì¸êµ¬' in col]\n",
    "    print(f\"   ğŸ“‹ ìƒì£¼ì¸êµ¬ ê´€ë ¨ ì»¬ëŸ¼: {resident_pop_cols[:5]}\")\n",
    "    \n",
    "    if resident_pop_cols:\n",
    "        # ì²« ë²ˆì§¸ ì¸êµ¬ ê´€ë ¨ ì»¬ëŸ¼ì„ ìƒì£¼ì¸êµ¬ë¡œ ì‚¬ìš©\n",
    "        main_resident_col = resident_pop_cols[0]\n",
    "        resident_latest['ìƒì£¼ì¸êµ¬_ìˆ˜'] = resident_latest[main_resident_col]\n",
    "        print(f\"   ğŸ“‹ ì‚¬ìš©í•  ìƒì£¼ì¸êµ¬ ì»¬ëŸ¼: {main_resident_col}\")\n",
    "        \n",
    "        resident_selected = resident_latest[['TRDAR_CD', 'ìƒì£¼ì¸êµ¬_ìˆ˜']].copy()\n",
    "        master_df = master_df.merge(resident_selected, on='TRDAR_CD', how='left')\n",
    "        \n",
    "        print(f\"   ğŸ“ˆ ìƒì£¼ì¸êµ¬ ë§¤ì¹­ë¥ : {(master_df['ìƒì£¼ì¸êµ¬_ìˆ˜'].notna().sum() / len(master_df)):.1%}\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ ìƒì£¼ì¸êµ¬ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ\")\n",
    "else:\n",
    "    print(f\"   âš ï¸ ìƒì£¼ì¸êµ¬ ë°ì´í„° ì¡°ì¸ ê±´ë„ˆëœ€\")\n",
    "\n",
    "# 4. ì¶”ì •ë§¤ì¶œ ë°ì´í„° ì¡°ì¸\n",
    "print(f\"\\n4ï¸âƒ£ ì¶”ì •ë§¤ì¶œ ë°ì´í„° ì¡°ì¸:\")\n",
    "\n",
    "if standardize_district_code(sales_latest, 'sales'):\n",
    "    # ì¶”ì •ë§¤ì¶œ ê´€ë ¨ ì»¬ëŸ¼ ì°¾ê¸°\n",
    "    sales_cols = [col for col in sales_latest.columns if 'ë§¤ì¶œ' in col or 'SAL' in col or 'SALES' in col]\n",
    "    print(f\"   ğŸ“‹ ë§¤ì¶œ ê´€ë ¨ ì»¬ëŸ¼: {sales_cols[:5]}\")\n",
    "    \n",
    "    if sales_cols:\n",
    "        main_sales_col = sales_cols[0]\n",
    "        sales_latest['ì¶”ì •ë§¤ì¶œ'] = sales_latest[main_sales_col]\n",
    "        print(f\"   ğŸ“‹ ì‚¬ìš©í•  ë§¤ì¶œ ì»¬ëŸ¼: {main_sales_col}\")\n",
    "        \n",
    "        sales_selected = sales_latest[['TRDAR_CD', 'ì¶”ì •ë§¤ì¶œ']].copy()\n",
    "        master_df = master_df.merge(sales_selected, on='TRDAR_CD', how='left')\n",
    "        \n",
    "        print(f\"   ğŸ“ˆ ì¶”ì •ë§¤ì¶œ ë§¤ì¹­ë¥ : {(master_df['ì¶”ì •ë§¤ì¶œ'].notna().sum() / len(master_df)):.1%}\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ ë§¤ì¶œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ\")\n",
    "else:\n",
    "    print(f\"   âš ï¸ ì¶”ì •ë§¤ì¶œ ë°ì´í„° ì¡°ì¸ ê±´ë„ˆëœ€\")\n",
    "\n",
    "# 5. ì†Œë“Â·ì†Œë¹„ ë°ì´í„° ì¡°ì¸\n",
    "print(f\"\\n5ï¸âƒ£ ì†Œë“Â·ì†Œë¹„ ë°ì´í„° ì¡°ì¸:\")\n",
    "\n",
    "if standardize_district_code(income_latest, 'income'):\n",
    "    # ì†Œë“Â·ì†Œë¹„ ê´€ë ¨ ì»¬ëŸ¼ ì°¾ê¸°\n",
    "    income_cols = [col for col in income_latest.columns if 'ì†Œë“' in col or 'INCOME' in col]\n",
    "    expense_cols = [col for col in income_latest.columns if 'ì†Œë¹„' in col or 'EXPEND' in col]\n",
    "    \n",
    "    print(f\"   ğŸ“‹ ì†Œë“ ê´€ë ¨ ì»¬ëŸ¼: {income_cols[:3]}\")\n",
    "    print(f\"   ğŸ“‹ ì†Œë¹„ ê´€ë ¨ ì»¬ëŸ¼: {expense_cols[:3]}\")\n",
    "    \n",
    "    income_data = {}\n",
    "    if income_cols:\n",
    "        income_data['í‰ê· ì†Œë“'] = income_latest[income_cols[0]]\n",
    "    if expense_cols:\n",
    "        income_data['í‰ê· ì†Œë¹„'] = income_latest[expense_cols[0]]\n",
    "    \n",
    "    if income_data:\n",
    "        income_selected = pd.DataFrame({'TRDAR_CD': income_latest['TRDAR_CD']})\n",
    "        for key, series in income_data.items():\n",
    "            income_selected[key] = series\n",
    "        \n",
    "        master_df = master_df.merge(income_selected, on='TRDAR_CD', how='left')\n",
    "        print(f\"   ğŸ“ˆ ì†Œë“Â·ì†Œë¹„ ë§¤ì¹­ë¥ : {(master_df[list(income_data.keys())[0]].notna().sum() / len(master_df)):.1%}\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ ì†Œë“Â·ì†Œë¹„ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ\")\n",
    "else:\n",
    "    print(f\"   âš ï¸ ì†Œë“Â·ì†Œë¹„ ë°ì´í„° ì¡°ì¸ ê±´ë„ˆëœ€\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ë§ˆìŠ¤í„° ë°ì´í„°ì…‹ ê¸°ë³¸ êµ¬ì¶• ì™„ë£Œ: {master_df.shape}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# ================================================================================\n",
    "# ğŸ“Š íŒŒìƒì§€í‘œ ìƒì„±\n",
    "# ================================================================================\n",
    "\n",
    "print(\"ğŸ“Š ê³±ì°½ì§‘ íŠ¹í™” íŒŒìƒì§€í‘œ ìƒì„±\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 1. ì¸êµ¬ë°€ë„ ì§€í‘œ\n",
    "print(\"1ï¸âƒ£ ì¸êµ¬ë°€ë„ ì§€í‘œ:\")\n",
    "master_df['ìœ ë™ë°€ë„_ì ìˆ˜'] = pd.qcut(master_df['ì´_ìœ ë™ì¸êµ¬_ìˆ˜'].fillna(0), \n",
    "                                  q=5, labels=[1,2,3,4,5]).astype(float)\n",
    "\n",
    "master_df['ì•¼ê°„ë°€ë„_ì ìˆ˜'] = pd.qcut(master_df['ì•¼ê°„_ìœ ë™ì¸êµ¬_17_24'].fillna(0), \n",
    "                                   q=5, labels=[1,2,3,4,5]).astype(float)\n",
    "\n",
    "print(f\"   âœ… ìœ ë™ë°€ë„ ì ìˆ˜: 1(ìµœì €) ~ 5(ìµœê³ )\")\n",
    "print(f\"   âœ… ì•¼ê°„ë°€ë„ ì ìˆ˜: 1(ìµœì €) ~ 5(ìµœê³ )\")\n",
    "\n",
    "# 2. ë§¤ì¶œ íš¨ìœ¨ì„± ì§€í‘œ\n",
    "print(f\"\\n2ï¸âƒ£ ë§¤ì¶œ íš¨ìœ¨ì„± ì§€í‘œ:\")\n",
    "master_df['ì´ì¸êµ¬'] = master_df['ì´_ìœ ë™ì¸êµ¬_ìˆ˜'].fillna(0) + master_df['ìƒì£¼ì¸êµ¬_ìˆ˜'].fillna(0)\n",
    "\n",
    "# ì¸êµ¬ 1ëª…ë‹¹ ì¶”ì •ë§¤ì¶œ\n",
    "master_df['ì¸ë‹¹_ë§¤ì¶œíš¨ìœ¨'] = master_df['ì¶”ì •ë§¤ì¶œ'] / master_df['ì´ì¸êµ¬']\n",
    "master_df['ì¸ë‹¹_ë§¤ì¶œíš¨ìœ¨'] = master_df['ì¸ë‹¹_ë§¤ì¶œíš¨ìœ¨'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# ì•¼ê°„ íŠ¹í™” íš¨ìœ¨ì„±\n",
    "master_df['ì•¼ê°„_ë§¤ì¶œíš¨ìœ¨'] = master_df['ì¶”ì •ë§¤ì¶œ'] / master_df['ì•¼ê°„_ìœ ë™ì¸êµ¬_17_24']\n",
    "master_df['ì•¼ê°„_ë§¤ì¶œíš¨ìœ¨'] = master_df['ì•¼ê°„_ë§¤ì¶œíš¨ìœ¨'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "print(f\"   âœ… ì¸ë‹¹ ë§¤ì¶œíš¨ìœ¨: í‰ê·  {master_df['ì¸ë‹¹_ë§¤ì¶œíš¨ìœ¨'].mean():.0f}ì›/ëª…\")\n",
    "print(f\"   âœ… ì•¼ê°„ ë§¤ì¶œíš¨ìœ¨: í‰ê·  {master_df['ì•¼ê°„_ë§¤ì¶œíš¨ìœ¨'].mean():.0f}ì›/ëª…\")\n",
    "\n",
    "# 3. ê³±ì°½ì§‘ íŠ¹í™” ì§€í‘œ\n",
    "print(f\"\\n3ï¸âƒ£ ê³±ì°½ì§‘ íŠ¹í™” ì§€í‘œ:\")\n",
    "\n",
    "# ì£¼ë¥˜ ì¹œí™” ì§€í‘œ (ì•¼ê°„ + ì£¼ë§ ê°€ì¤‘)\n",
    "master_df['ì£¼ë¥˜ì¹œí™”_ì§€ìˆ˜'] = (master_df['ì•¼ê°„_ë¹„ìœ¨'].fillna(0) * 0.6 + \n",
    "                           master_df['ì£¼ë§_í”„ë¦¬ë¯¸ì—„'].fillna(1) * 0.4)\n",
    "\n",
    "# ì—°ë ¹ëŒ€ë³„ íƒ€ê²Ÿ ê³ ê° ë¹„ìœ¨ (20-40ëŒ€)\n",
    "target_age_cols = ['ì—°ë ¹ëŒ€_20_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì—°ë ¹ëŒ€_30_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì—°ë ¹ëŒ€_40_ìœ ë™ì¸êµ¬_ìˆ˜']\n",
    "available_age_cols = [col for col in target_age_cols if col in master_df.columns]\n",
    "\n",
    "if available_age_cols:\n",
    "    master_df['íƒ€ê²Ÿì—°ë ¹_ë¹„ìœ¨'] = master_df[available_age_cols].sum(axis=1) / master_df['ì´_ìœ ë™ì¸êµ¬_ìˆ˜']\n",
    "    master_df['íƒ€ê²Ÿì—°ë ¹_ë¹„ìœ¨'] = master_df['íƒ€ê²Ÿì—°ë ¹_ë¹„ìœ¨'].fillna(0)\n",
    "    print(f\"   âœ… íƒ€ê²Ÿì—°ë ¹(20-40ëŒ€) ë¹„ìœ¨: í‰ê·  {master_df['íƒ€ê²Ÿì—°ë ¹_ë¹„ìœ¨'].mean():.1%}\")\n",
    "\n",
    "# ë‚¨ë…€ ë¹„ìœ¨ ê· í˜•ë„ (ê³±ì°½ì€ ë‚¨ë…€ ëª¨ë‘ ì„ í˜¸)\n",
    "if all(col in master_df.columns for col in ['ë‚¨ì„±_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì—¬ì„±_ìœ ë™ì¸êµ¬_ìˆ˜']):\n",
    "    master_df['ì„±ë¹„_ê· í˜•ë„'] = 1 - abs(master_df['ë‚¨ì„±_ìœ ë™ì¸êµ¬_ìˆ˜'] - master_df['ì—¬ì„±_ìœ ë™ì¸êµ¬_ìˆ˜']) / master_df['ì´_ìœ ë™ì¸êµ¬_ìˆ˜']\n",
    "    master_df['ì„±ë¹„_ê· í˜•ë„'] = master_df['ì„±ë¹„_ê· í˜•ë„'].fillna(0.5)\n",
    "    print(f\"   âœ… ì„±ë¹„ ê· í˜•ë„: í‰ê·  {master_df['ì„±ë¹„_ê· í˜•ë„'].mean():.1%}\")\n",
    "\n",
    "print(f\"   âœ… ì£¼ë¥˜ì¹œí™” ì§€ìˆ˜: í‰ê·  {master_df['ì£¼ë¥˜ì¹œí™”_ì§€ìˆ˜'].mean():.2f}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š íŒŒìƒì§€í‘œ ìƒì„± ì™„ë£Œ: {len([col for col in master_df.columns if 'ì ìˆ˜' in col or 'ì§€ìˆ˜' in col or 'íš¨ìœ¨' in col])}ê°œ\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# ================================================================================\n",
    "# ğŸ—ºï¸ ì§€ë¦¬ì •ë³´ ì²˜ë¦¬ ë° GeoPackage ìƒì„±\n",
    "# ================================================================================\n",
    "\n",
    "print(\"ğŸ—ºï¸ ì§€ë¦¬ì •ë³´ ì²˜ë¦¬ ë° GeoPackage ìƒì„±\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# ì§€ë¦¬ì •ë³´ ì»¬ëŸ¼ í™•ì¸\n",
    "geo_columns = [col for col in master_df.columns if any(keyword in col.lower() \n",
    "              for keyword in ['ê²½ë„', 'lng', 'lon', 'ìœ„ë„', 'lat', 'xì¢Œí‘œ', 'yì¢Œí‘œ'])]\n",
    "\n",
    "print(f\"ğŸ“ ì§€ë¦¬ì •ë³´ ì»¬ëŸ¼: {geo_columns}\")\n",
    "\n",
    "if len(geo_columns) >= 2:\n",
    "    # ê²½ë„/ìœ„ë„ ì»¬ëŸ¼ ì‹ë³„\n",
    "    lng_col = next((col for col in geo_columns if 'ê²½ë„' in col or 'lng' in col.lower() or 'lon' in col.lower()), geo_columns[0])\n",
    "    lat_col = next((col for col in geo_columns if 'ìœ„ë„' in col or 'lat' in col.lower()), geo_columns[1])\n",
    "    \n",
    "    print(f\"   ğŸ“ ê²½ë„ ì»¬ëŸ¼: {lng_col}\")\n",
    "    print(f\"   ğŸ“ ìœ„ë„ ì»¬ëŸ¼: {lat_col}\")\n",
    "    \n",
    "    # ìœ íš¨í•œ ì¢Œí‘œ ë°ì´í„° í™•ì¸\n",
    "    valid_coords = master_df[lng_col].notna() & master_df[lat_col].notna()\n",
    "    valid_count = valid_coords.sum()\n",
    "    \n",
    "    print(f\"   ğŸ“Š ìœ íš¨ ì¢Œí‘œ: {valid_count:,}ê°œ ({valid_count/len(master_df):.1%})\")\n",
    "    \n",
    "    if valid_count > 0:\n",
    "        # GeoDataFrame ìƒì„±\n",
    "        geo_master = master_df[valid_coords].copy()\n",
    "        \n",
    "        try:\n",
    "            from shapely.geometry import Point\n",
    "            \n",
    "            # Point ì§€ì˜¤ë©”íŠ¸ë¦¬ ìƒì„±\n",
    "            geometry = [Point(lng, lat) for lng, lat in zip(geo_master[lng_col], geo_master[lat_col])]\n",
    "            geo_master_gdf = gpd.GeoDataFrame(geo_master, geometry=geometry, crs='EPSG:4326')\n",
    "            \n",
    "            print(f\"   âœ… GeoDataFrame ìƒì„± ì„±ê³µ: {len(geo_master_gdf):,}ê°œ ìƒê¶Œ\")\n",
    "            \n",
    "            # GeoPackage ì €ì¥\n",
    "            output_path = DATA_PATH / \"processed\"\n",
    "            output_path.mkdir(exist_ok=True)\n",
    "            \n",
    "            gpkg_file = output_path / \"seoul_gopchang_master.gpkg\"\n",
    "            geo_master_gdf.to_file(gpkg_file, driver='GPKG')\n",
    "            \n",
    "            print(f\"   ğŸ’¾ GeoPackage ì €ì¥: {gpkg_file}\")\n",
    "            print(f\"   ğŸ“ ì¢Œí‘œê³„: EPSG:4326 (WGS84)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ GeoDataFrame ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "            geo_master_gdf = None\n",
    "    else:\n",
    "        print(f\"   âš ï¸ ìœ íš¨í•œ ì¢Œí‘œ ë°ì´í„° ë¶€ì¡±\")\n",
    "        geo_master_gdf = None\n",
    "else:\n",
    "    print(f\"   âš ï¸ ì§€ë¦¬ì •ë³´ ì»¬ëŸ¼ ë¶€ì¡± (CSVë§Œ ì €ì¥)\")\n",
    "    geo_master_gdf = None\n",
    "\n",
    "# CSV ë°±ì—… ì €ì¥\n",
    "csv_file = DATA_PATH / \"processed\" / \"seoul_gopchang_master.csv\"\n",
    "master_df.to_csv(csv_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"   ğŸ’¾ CSV ë°±ì—… ì €ì¥: {csv_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# ================================================================================\n",
    "# ğŸ“Š ë§ˆìŠ¤í„° ë°ì´í„°ì…‹ í’ˆì§ˆ ê²€ì¦ ë° ìš”ì•½\n",
    "# ================================================================================\n",
    "\n",
    "print(\"ğŸ“Š ë§ˆìŠ¤í„° ë°ì´í„°ì…‹ í’ˆì§ˆ ê²€ì¦ ë° ìš”ì•½\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 1. ê¸°ë³¸ í†µê³„\n",
    "print(\"1ï¸âƒ£ ê¸°ë³¸ í†µê³„:\")\n",
    "print(f\"   ğŸ“Š ì´ ìƒê¶Œ ìˆ˜: {len(master_df):,}ê°œ\")\n",
    "print(f\"   ğŸ“‹ ì´ ì»¬ëŸ¼ ìˆ˜: {len(master_df.columns)}ê°œ\")\n",
    "\n",
    "# 2. í•µì‹¬ ì§€í‘œë³„ ê²°ì¸¡ë¥ \n",
    "print(f\"\\n2ï¸âƒ£ í•µì‹¬ ì§€í‘œ ê²°ì¸¡ë¥ :\")\n",
    "key_columns = ['ì´_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì•¼ê°„_ìœ ë™ì¸êµ¬_17_24', 'ìƒì£¼ì¸êµ¬_ìˆ˜', 'ì¶”ì •ë§¤ì¶œ']\n",
    "\n",
    "for col in key_columns:\n",
    "    if col in master_df.columns:\n",
    "        missing_rate = master_df[col].isna().sum() / len(master_df)\n",
    "        print(f\"   {col:20s}: {missing_rate:6.1%} ê²°ì¸¡\")\n",
    "\n",
    "# 3. ì•¼ê°„ ìœ ë™ì¸êµ¬ ë¶„í¬\n",
    "print(f\"\\n3ï¸âƒ£ ì•¼ê°„ ìœ ë™ì¸êµ¬ ë¶„í¬ (17-24ì‹œ):\")\n",
    "if 'ì•¼ê°„_ìœ ë™ì¸êµ¬_17_24' in master_df.columns:\n",
    "    night_stats = master_df['ì•¼ê°„_ìœ ë™ì¸êµ¬_17_24'].describe()\n",
    "    for stat, value in night_stats.items():\n",
    "        if stat in ['mean', 'std', '50%', 'max']:\n",
    "            print(f\"   {stat:8s}: {value:10,.0f}ëª…\")\n",
    "\n",
    "# 4. ìƒìœ„ ìƒê¶Œ ë¯¸ë¦¬ë³´ê¸°\n",
    "print(f\"\\n4ï¸âƒ£ ì•¼ê°„ ìœ ë™ì¸êµ¬ TOP 10 ìƒê¶Œ:\")\n",
    "if 'ì•¼ê°„_ìœ ë™ì¸êµ¬_17_24' in master_df.columns and 'ìƒê¶Œ_ì½”ë“œ_ëª…' in master_df.columns:\n",
    "    top_districts = master_df.nlargest(10, 'ì•¼ê°„_ìœ ë™ì¸êµ¬_17_24')[['ìƒê¶Œ_ì½”ë“œ_ëª…', 'ì•¼ê°„_ìœ ë™ì¸êµ¬_17_24']]\n",
    "    for i, (idx, row) in enumerate(top_districts.iterrows(), 1):\n",
    "        print(f\"   {i:2d}. {row['ìƒê¶Œ_ì½”ë“œ_ëª…']:20s}: {row['ì•¼ê°„_ìœ ë™ì¸êµ¬_17_24']:7,.0f}ëª…\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# ================================================================================\n",
    "# ğŸ“ˆ ë°ì´í„° í’ˆì§ˆ ì‹œê°í™”\n",
    "# ================================================================================\n",
    "\n",
    "print(\"ğŸ“ˆ ë§ˆìŠ¤í„° ë°ì´í„°ì…‹ í’ˆì§ˆ ì‹œê°í™”\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 1. ê²°ì¸¡ë¥  íˆíŠ¸ë§µ\n",
    "missing_rates = master_df.isnull().sum() / len(master_df)\n",
    "important_cols = missing_rates[missing_rates > 0].head(15)\n",
    "\n",
    "if len(important_cols) > 0:\n",
    "    fig_missing = go.Figure(data=go.Bar(\n",
    "        y=important_cols.index[::-1],  # ì—­ìˆœìœ¼ë¡œ í‘œì‹œ\n",
    "        x=important_cols.values[::-1] * 100,\n",
    "        orientation='h',\n",
    "        marker_color='lightcoral',\n",
    "        text=[f'{val:.1f}%' for val in important_cols.values[::-1] * 100],\n",
    "        textposition='auto'\n",
    "    ))\n",
    "    \n",
    "    fig_missing.update_layout(\n",
    "        title=\"ğŸ“Š ì£¼ìš” ì»¬ëŸ¼ë³„ ê²°ì¸¡ë¥ \",\n",
    "        xaxis_title=\"ê²°ì¸¡ë¥  (%)\",\n",
    "        yaxis_title=\"ì»¬ëŸ¼ëª…\",\n",
    "        height=500,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig_missing.show()\n",
    "\n",
    "# 2. ì•¼ê°„ ìœ ë™ì¸êµ¬ ë¶„í¬\n",
    "if 'ì•¼ê°„_ìœ ë™ì¸êµ¬_17_24' in master_df.columns:\n",
    "    night_flow_clean = master_df['ì•¼ê°„_ìœ ë™ì¸êµ¬_17_24'].dropna()\n",
    "    \n",
    "    fig_dist = go.Figure()\n",
    "    \n",
    "    fig_dist.add_trace(go.Histogram(\n",
    "        x=night_flow_clean,\n",
    "        nbinsx=50,\n",
    "        name='ì•¼ê°„ ìœ ë™ì¸êµ¬',\n",
    "        marker_color='lightblue',\n",
    "        opacity=0.7\n",
    "    ))\n",
    "    \n",
    "    fig_dist.update_layout(\n",
    "        title=\"ğŸŒ™ ì•¼ê°„ ìœ ë™ì¸êµ¬ ë¶„í¬ (17-24ì‹œ)\",\n",
    "        xaxis_title=\"ì•¼ê°„ ìœ ë™ì¸êµ¬ (ëª…)\",\n",
    "        yaxis_title=\"ìƒê¶Œ ìˆ˜\",\n",
    "        height=400,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig_dist.show()\n",
    "\n",
    "# 3. ì¢…í•© ëŒ€ì‹œë³´ë“œ\n",
    "fig_summary = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('ë°ì´í„° ë¡œë“œ í˜„í™©', 'í•µì‹¬ ì§€í‘œ í†µê³„', 'ìƒìœ„ ìƒê¶Œ ë¶„í¬', 'í’ˆì§ˆ ì ìˆ˜'),\n",
    "    specs=[[{\"type\": \"pie\"}, {\"type\": \"bar\"}],\n",
    "           [{\"type\": \"scatter\"}, {\"type\": \"table\"}]]\n",
    ")\n",
    "\n",
    "# 3-1. ë°ì´í„° ë¡œë“œ ì„±ê³µë¥ \n",
    "success_data = [1 if 'âœ…' in status else 0 for status in data_status.values() if 'âœ…' in status or 'âŒ' in status]\n",
    "success_rate = sum(success_data) / len(success_data) if success_data else 0\n",
    "\n",
    "fig_summary.add_trace(\n",
    "    go.Pie(labels=['ì„±ê³µ', 'ì‹¤íŒ¨'], values=[success_rate, 1-success_rate],\n",
    "           marker_colors=['lightgreen', 'lightcoral']),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 3-2. í•µì‹¬ ì§€í‘œ í†µê³„\n",
    "key_stats = {}\n",
    "for col in ['ì´_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì•¼ê°„_ìœ ë™ì¸êµ¬_17_24', 'ìƒì£¼ì¸êµ¬_ìˆ˜']:\n",
    "    if col in master_df.columns:\n",
    "        key_stats[col.replace('_', ' ')] = master_df[col].mean()\n",
    "\n",
    "if key_stats:\n",
    "    fig_summary.add_trace(\n",
    "        go.Bar(x=list(key_stats.keys()), y=list(key_stats.values()),\n",
    "               marker_color='lightblue'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# 3-3. ìƒê¶Œ ë¶„í¬ (ì§€ì—­ë³„ - ìƒê¶Œêµ¬ë¶„ì½”ë“œ ê¸°ì¤€)\n",
    "if 'ìƒê¶Œ_êµ¬ë¶„_ì½”ë“œ_ëª…' in master_df.columns:\n",
    "    district_counts = master_df['ìƒê¶Œ_êµ¬ë¶„_ì½”ë“œ_ëª…'].value_counts().head(10)\n",
    "    \n",
    "    fig_summary.add_trace(\n",
    "        go.Scatter(x=list(range(len(district_counts))), y=district_counts.values,\n",
    "                  mode='markers+lines', marker_size=10, marker_color='orange'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# 3-4. í’ˆì§ˆ ìš”ì•½ í…Œì´ë¸”\n",
    "quality_summary = [\n",
    "    ['ì´ ìƒê¶Œ ìˆ˜', f'{len(master_df):,}ê°œ'],\n",
    "    ['ì™„ì „ë°ì´í„° ìƒê¶Œ', f'{master_df.dropna().shape[0]:,}ê°œ'],\n",
    "    ['ì•¼ê°„ë°ì´í„° ë³´ìœ ', f'{master_df[\"ì•¼ê°„_ìœ ë™ì¸êµ¬_17_24\"].notna().sum():,}ê°œ'],\n",
    "    ['í‰ê·  ì•¼ê°„ ìœ ë™ì¸êµ¬', f'{master_df[\"ì•¼ê°„_ìœ ë™ì¸êµ¬_17_24\"].mean():,.0f}ëª…'],\n",
    "    ['ì§€ë¦¬ì •ë³´ ë³´ìœ ', f'{valid_count if \"valid_count\" in locals() else 0:,}ê°œ']\n",
    "]\n",
    "\n",
    "fig_summary.add_trace(\n",
    "    go.Table(\n",
    "        header=dict(values=['ì§€í‘œ', 'ê°’'], fill_color='lightblue', align='center'),\n",
    "        cells=dict(values=list(zip(*quality_summary)), fill_color='white', align='center')),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig_summary.update_layout(\n",
    "    title=\"ğŸ¯ ê³±ì°½ì§‘ ë§ˆìŠ¤í„° ë°ì´í„°ì…‹ ì¢…í•© í˜„í™©\",\n",
    "    height=800,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig_summary.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# ================================================================================\n",
    "# ğŸ¯ ì™„ë£Œ ë° ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´\n",
    "# ================================================================================\n",
    "\n",
    "print(\"ğŸ¯ ë§ˆìŠ¤í„° ë°ì´í„°ì…‹ êµ¬ì¶• ì™„ë£Œ!\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(f\"âœ… ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nğŸ“Š ìµœì¢… ê²°ê³¼:\")\n",
    "print(f\"   ğŸ—ƒï¸  ë§ˆìŠ¤í„° ë°ì´í„°ì…‹: {master_df.shape}\")\n",
    "print(f\"   ğŸŒ™ ì•¼ê°„ ìœ ë™ì¸êµ¬ (17-24ì‹œ): {master_df['ì•¼ê°„_ìœ ë™ì¸êµ¬_17_24'].notna().sum():,}ê°œ ìƒê¶Œ\")\n",
    "print(f\"   ğŸ’¾ ì €ì¥ íŒŒì¼: seoul_gopchang_master.csv / .gpkg\")\n",
    "print(f\"   ğŸ“ ì§€ë¦¬ì •ë³´: {valid_count if 'valid_count' in locals() else 0:,}ê°œ ìƒê¶Œ\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„ ì˜µì…˜:\")\n",
    "print(f\"   A. KPI ìŠ¤ì½”ì–´ë§ (ê²½ìŸë°€ë„, ì„ëŒ€ë£Œ, í†µí•©ì ìˆ˜)\")\n",
    "print(f\"   B. ìƒê¶Œë³„ ìƒì„¸ ë¶„ì„ (TOP 30 ì¶”ì¶œ)\")\n",
    "print(f\"   C. ì§€ë„ ì‹œê°í™” (Folium ì¸í„°ë™í‹°ë¸Œ ë§µ)\")\n",
    "print(f\"   D. ëŒ€ì‹œë³´ë“œ êµ¬ì¶• (Plotly Dash)\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ ê¶Œì¥: Aë²ˆ (KPI ìŠ¤ì½”ì–´ë§)ë¶€í„° ì§„í–‰\")\n",
    "print(f\"   ì´ìœ : ë§ˆìŠ¤í„° ë°ì´í„°ê°€ ì™„ì„±ë˜ì—ˆìœ¼ë‹ˆ ê³±ì°½ì§‘ íŠ¹í™” ì ìˆ˜ ê³„ì‚° ê°€ëŠ¥\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ—ï¸ ë§ˆìŠ¤í„° ë°ì´í„°ì…‹ êµ¬ì¶• ì™„ë£Œ!\")\n",
    "print(\"ğŸš€ ì´ì œ ë³¸ê²©ì ì¸ ì…ì§€ ë¶„ì„ì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gopchang-locator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
