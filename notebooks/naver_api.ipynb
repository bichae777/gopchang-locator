{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud         # ê¸°ë³¸\n",
    "# conda í™˜ê²½ì„ ì“°ì‹ ë‹¤ë©´ â†˜\n",
    "# !conda install -c conda-forge wordcloud -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ– ë„¤ì´ë²„ API í™œìš© ê³±ì°½ì§‘ íŠ¸ë Œë“œ ë¶„ì„ ì‹œìŠ¤í…œ\n",
    "-------------------------------------------------\n",
    "* PythonÂ 3.9+\n",
    "* requests, pandas, numpy, matplotlib, seaborn, wordcloud\n",
    "* ì•ˆì „í•œ API í‚¤ ê´€ë¦¬ë¥¼ ìœ„í•´ `.env`, í™˜ê²½ ë³€ìˆ˜, Secret Manager ë“±ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n",
    "  (ì—¬ê¸°ì„œëŠ” ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ í‚¤ë¥¼ ì§ì ‘ íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬í•˜ë„ë¡ ì˜ˆì‹œ)\n",
    "\"\"\"\n",
    "\n",
    "# ========================\n",
    "# 1. ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "# ========================\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"AppleGothic\"  # macOS ê¸°ì¤€, Windows = \"Malgun Gothic\"\n",
    "\n",
    "# ========================\n",
    "# 2. ë„¤ì´ë²„ ê²€ìƒ‰ API í´ë¼ì´ì–¸íŠ¸\n",
    "# ========================\n",
    "class NaverAPIClient:\n",
    "    \"\"\"ë„¤ì´ë²„ ê²€ìƒ‰ API ë˜í¼ í´ë˜ìŠ¤\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    client_id : str\n",
    "        ë„¤ì´ë²„ ê°œë°œìì„¼í„° ClientÂ ID\n",
    "    client_secret : str\n",
    "        ë„¤ì´ë²„ ê°œë°œìì„¼í„° ClientÂ Secret\n",
    "    rate_limit : float, optional\n",
    "        í˜¸ì¶œ ê°„ ìµœì†Œ ëŒ€ê¸° ì‹œê°„(ì´ˆ). ê¸°ë³¸ 0.2Â ì´ˆ.\n",
    "    \"\"\"\n",
    "\n",
    "    BASE_URL = \"https://openapi.naver.com/v1/search\"\n",
    "\n",
    "    def __init__(self, client_id: str, client_secret: str, *, rate_limit: float = 0.2):\n",
    "        self.client_id = client_id\n",
    "        self.client_secret = client_secret\n",
    "        self.rate_limit = rate_limit\n",
    "        self.headers = {\n",
    "            \"X-Naver-Client-Id\": self.client_id,\n",
    "            \"X-Naver-Client-Secret\": self.client_secret,\n",
    "        }\n",
    "\n",
    "    # ------------------------\n",
    "    # ë‚´ë¶€ í—¬í¼: ìš”ì²­ & ì˜ˆì™¸ ì²˜ë¦¬\n",
    "    # ------------------------\n",
    "    def _request(self, endpoint: str, params: Dict[str, str]) -> Dict:\n",
    "        url = f\"{self.BASE_URL}/{endpoint}.json\"\n",
    "        resp = requests.get(url, headers=self.headers, params=params, timeout=10)\n",
    "        if resp.status_code != 200:\n",
    "            raise RuntimeError(\n",
    "                f\"Naver API error {resp.status_code}: {resp.text[:200]}â€¦\"\n",
    "            )\n",
    "        time.sleep(self.rate_limit)\n",
    "        return resp.json()\n",
    "\n",
    "    # ------------------------\n",
    "    # í¼ë¸”ë¦­: ë‰´ìŠ¤Â·ë¸”ë¡œê·¸Â·ì¹´í˜ ê²€ìƒ‰\n",
    "    # ------------------------\n",
    "    def search(self, query: str, *, source: str, display: int = 100, max_pages: int = 1) -> List[Dict]:\n",
    "        \"\"\"ì§€ì • source(news/blog/cafearticle)ì—ì„œ ìµœëŒ€ `max_pages` Ã—Â `display` ê±´ ìˆ˜ì§‘\"\"\"\n",
    "        items: List[Dict] = []\n",
    "        for page in range(max_pages):\n",
    "            start = page * display + 1\n",
    "            payload = {\n",
    "                \"query\": query,\n",
    "                \"display\": display,\n",
    "                \"start\": start,\n",
    "                \"sort\": \"date\",  # ìµœì‹ ìˆœ\n",
    "            }\n",
    "            data = self._request(source, payload)\n",
    "            items.extend(data.get(\"items\", []))\n",
    "            if len(data.get(\"items\", [])) < display:\n",
    "                break  # ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬\n",
    "        return items\n",
    "\n",
    "    # í¸ì˜ ë©”ì„œë“œ\n",
    "    def search_news(self, query: str, **kw):\n",
    "        return self.search(query, source=\"news\", **kw)\n",
    "\n",
    "    def search_blog(self, query: str, **kw):\n",
    "        return self.search(query, source=\"blog\", **kw)\n",
    "\n",
    "    def search_cafe(self, query: str, **kw):\n",
    "        return self.search(query, source=\"cafearticle\", **kw)\n",
    "\n",
    "\n",
    "# ========================\n",
    "# 3. ê³±ì°½ íŠ¸ë Œë“œ ë¶„ì„ê¸°\n",
    "# ========================\n",
    "class GopchangTrendAnalyzer:\n",
    "    \"\"\"ê³±ì°½ í‚¤ì›Œë“œ & ì§€ì—­ ê¸°ë°˜ íŠ¸ë Œë“œ í¬ë¡¤Â·ë¶„ì„\"\"\"\n",
    "\n",
    "    BASE_KEYWORDS = [\n",
    "        \"ê³±ì°½\",\n",
    "        \"ë§‰ì°½\",\n",
    "        \"ëŒ€ì°½\",\n",
    "        \"ì–‘ê³±ì°½\",\n",
    "        \"ê³±ì°½ì „ê³¨\",\n",
    "        \"ê³±ì°½êµ¬ì´\",\n",
    "        \"í•œìš°ê³±ì°½\",\n",
    "    ]\n",
    "\n",
    "    TOP_DISTRICTS = [\n",
    "        \"ê°•ë‚¨ì—­\",\n",
    "        \"ì¢…ë¡œ\",\n",
    "        \"ëª…ë™\",\n",
    "        \"ì‹ ì´Œ\",\n",
    "        \"í™ëŒ€\",\n",
    "        \"ë…¸ëŸ‰ì§„\",\n",
    "        \"ë…¸ì›\",\n",
    "        \"ì ì‹¤\",\n",
    "        \"ì—¬ì˜ë„\",\n",
    "        \"ì—°ë‚¨ë™\",\n",
    "    ]\n",
    "\n",
    "    POSITIVE = [\n",
    "        \"ë§›ìˆ\",\n",
    "        \"ì¢‹\",\n",
    "        \"í›Œë¥­\",\n",
    "        \"ìµœê³ \",\n",
    "        \"ëŒ€ë°•\",\n",
    "        \"ì¶”ì²œ\",\n",
    "        \"ë§Œì¡±\",\n",
    "        \"ì‹ ì„ \",\n",
    "        \"ë¶€ë“œëŸ¬\",\n",
    "        \"ì«„ê¹ƒ\",\n",
    "        \"ê³ ì†Œ\",\n",
    "        \"ë‹¬ì½¤\",\n",
    "        \"ê°ë™\",\n",
    "    ]\n",
    "    NEGATIVE = [\n",
    "        \"ë§›ì—†\",\n",
    "        \"ë³„ë¡œ\",\n",
    "        \"ì‹¤ë§\",\n",
    "        \"ìµœì•…\",\n",
    "        \"ë¹„ì¶”ì²œ\",\n",
    "        \"ë¶ˆë§Œì¡±\",\n",
    "        \"ì§ˆê²¨\",\n",
    "        \"ì‹±ê±°\",\n",
    "        \"ì§œ\",\n",
    "        \"ë¹„ì‹¸\",\n",
    "        \"ë¶ˆì¹œì ˆ\",\n",
    "        \"ë”ëŸ¬\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, client: NaverAPIClient):\n",
    "        self.client = client\n",
    "\n",
    "    # ---------------------\n",
    "    # ë°ì´í„° ìˆ˜ì§‘\n",
    "    # ---------------------\n",
    "    def collect(self, days: int = 30) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"BASE_KEYWORDS + ì§€ì—­ ì¡°í•© í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤Â·ë¸”ë¡œê·¸Â·ì¹´í˜ ë°ì´í„° ìˆ˜ì§‘\"\"\"\n",
    "        print(\"ğŸ” Collecting trend dataâ€¦\")\n",
    "        cutoff_date = datetime.now() - pd.Timedelta(days=days)\n",
    "\n",
    "        data = {\"news\": [], \"blog\": [], \"cafe\": []}\n",
    "\n",
    "        # 1) ê¸°ë³¸ í‚¤ì›Œë“œ 3ê°œë§Œ (APIÂ ì¿¼í„° ê³ ë ¤)\n",
    "        for kw in self.BASE_KEYWORDS[:3]:\n",
    "            data[\"news\"].extend(self.client.search_news(kw, display=100, max_pages=1))\n",
    "            data[\"blog\"].extend(self.client.search_blog(kw, display=100, max_pages=1))\n",
    "\n",
    "        # 2) \"ì§€ì—­+í‚¤ì›Œë“œ\" ìƒìœ„ 10ê°œ ì¡°í•©ë§Œ ë¸”ë¡œê·¸ ìˆ˜ì§‘\n",
    "        combo_keywords = [f\"{d} {k}\" for d in self.TOP_DISTRICTS[:5] for k in self.BASE_KEYWORDS[:2]]\n",
    "        for kw in combo_keywords:\n",
    "            data[\"blog\"].extend(self.client.search_blog(kw, display=50, max_pages=1))\n",
    "\n",
    "        # 3) ë‚ ì§œ í•„í„°ë§ & ì •ê·œí™”\n",
    "        for src in data:\n",
    "            data[src] = [item for item in data[src] if self._is_recent(item, cutoff_date)]\n",
    "\n",
    "        print(\n",
    "            f\"âœ… Fetched: News={len(data['news'])}, Blog={len(data['blog'])}, Cafe={len(data['cafe'])}\"\n",
    "        )\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_recent(item: Dict, cutoff: datetime) -> bool:\n",
    "        try:\n",
    "            pub_date = datetime.strptime(item.get(\"pubDate\", \"\"), \"%a, %d %b %Y %H:%M:%S %z\")\n",
    "            return pub_date.replace(tzinfo=None) >= cutoff\n",
    "        except Exception:\n",
    "            return True  # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ í¬í•¨\n",
    "\n",
    "    # ---------------------\n",
    "    # ì „ì²˜ë¦¬ & ê°ì • ë¶„ì„\n",
    "    # ---------------------\n",
    "    def preprocess(self, raw: Dict[str, List[Dict]]) -> pd.DataFrame:\n",
    "        rows: List[Dict] = []\n",
    "        for src, items in raw.items():\n",
    "            for itm in items:\n",
    "                text = f\"{itm.get('title', '')} {itm.get('description', '')}\"\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"source\": src,\n",
    "                        \"title\": itm.get(\"title\", \"\"),\n",
    "                        \"description\": itm.get(\"description\", \"\"),\n",
    "                        \"link\": itm.get(\"link\", \"\"),\n",
    "                        \"pub_date\": itm.get(\"pubDate\", \"\"),\n",
    "                        \"sentiment\": self._sentiment(text),\n",
    "                        \"districts\": self._extract_districts(text),\n",
    "                    }\n",
    "                )\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def _sentiment(self, txt: str) -> str:\n",
    "        txt_clean = re.sub(r\"<[^>]+>\", \"\", txt)\n",
    "        pos = sum(word in txt_clean for word in self.POSITIVE)\n",
    "        neg = sum(word in txt_clean for word in self.NEGATIVE)\n",
    "        if pos > neg:\n",
    "            return \"positive\"\n",
    "        if neg > pos:\n",
    "            return \"negative\"\n",
    "        return \"neutral\"\n",
    "\n",
    "    def _extract_districts(self, txt: str) -> List[str]:\n",
    "        txt_clean = re.sub(r\"<[^>]+>\", \"\", txt)\n",
    "        return [d for d in self.TOP_DISTRICTS if d in txt_clean]\n",
    "\n",
    "    # ---------------------\n",
    "    # ìƒê¶Œë³„ ì¸ì‚¬ì´íŠ¸\n",
    "    # ---------------------\n",
    "    def district_insight(self, df: pd.DataFrame, district: str) -> str:\n",
    "        subset = df[df[\"districts\"].apply(lambda lst: district in lst if isinstance(lst, list) else False)]\n",
    "        if subset.empty:\n",
    "            return f\"âŒÂ {district} ì–¸ê¸‰ ì—†ìŒ\"\n",
    "\n",
    "        sentiment_ratio = (\n",
    "            subset[\"sentiment\"].value_counts(normalize=True).get(\"positive\", 0.0)\n",
    "        )\n",
    "        keywords = re.findall(r\"[ê°€-í£A-Za-z]+ì°½\", \" \".join(subset[\"title\"] + \" \" + subset[\"description\"]))\n",
    "        hot_kw = \", \".join([k for k, _ in Counter(keywords).most_common(3)])\n",
    "        main_src = subset[\"source\"].value_counts().idxmax()\n",
    "\n",
    "        return (\n",
    "            f\"ğŸ“ {district} íŠ¸ë Œë“œ\\n\"\n",
    "            f\"â€” ì´ ì–¸ê¸‰: {len(subset)}ê±´\\n\"\n",
    "            f\"â€” ê¸ì • ë¹„ìœ¨: {sentiment_ratio:.0%}\\n\"\n",
    "            f\"â€” ì¸ê¸° í‚¤ì›Œë“œ: {hot_kw or '-'}\\n\"\n",
    "            f\"â€” ì£¼ìš” ì†ŒìŠ¤: {main_src}\"\n",
    "        )\n",
    "\n",
    "# ========================\n",
    "# 4. íƒ€ê²Ÿ ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ê¸°\n",
    "# ========================\n",
    "class GopchangTargetAnalyzer:\n",
    "    SEGMENTS = {\n",
    "        \"ì§ì¥ì¸ íšŒì‹ì¡±\": {\n",
    "            \"íŠ¹ì§•\": [\"íšŒì‹\", \"ì•¼ì‹\", \"íšŒì‚¬\", \"ë™ë£Œ\", \"ìˆ \"],\n",
    "            \"ì‹œê°„\": \"18:00â€“22:00\",\n",
    "            \"ë©”ë‰´\": [\"ê³±ì°½êµ¬ì´\", \"ë§‰ì°½\", \"ì†Œì£¼\"],\n",
    "            \"ë¯¼ê°ë„\": \"ì¤‘ê°„\",\n",
    "        },\n",
    "        \"ì Šì€ì¸µ ë°ì´íŠ¸ì¡±\": {\n",
    "            \"íŠ¹ì§•\": [\"ë°ì´íŠ¸\", \"ì—°ì¸\", \"ë¶„ìœ„ê¸°\", \"ì¸ìŠ¤íƒ€\", \"ë§›ì§‘\"],\n",
    "            \"ì‹œê°„\": \"19:00â€“23:00\",\n",
    "            \"ë©”ë‰´\": [\"ê³±ì°½ì „ê³¨\", \"ì–‘ê³±ì°½\", \"ì™€ì¸\"],\n",
    "            \"ë¯¼ê°ë„\": \"ë‚®ìŒ\",\n",
    "        },\n",
    "        \"ê°€ì¡± ì™¸ì‹ì¡±\": {\n",
    "            \"íŠ¹ì§•\": [\"ê°€ì¡±\", \"ì•„ì´\", \"ì£¼ë§\", \"ë„“ì€\", \"ì ì‹¬\"],\n",
    "            \"ì‹œê°„\": \"12:00â€“14:00 / 18:00â€“20:00\",\n",
    "            \"ë©”ë‰´\": [\"ê³±ì°½ì „ê³¨\", \"í•œìš°ê³±ì°½\", \"ë°¥ë¥˜\"],\n",
    "            \"ë¯¼ê°ë„\": \"ë†’ìŒ\",\n",
    "        },\n",
    "        \"ê´€ê´‘ê°\": {\n",
    "            \"íŠ¹ì§•\": [\"ê´€ê´‘\", \"ì—¬í–‰\", \"í•œêµ­\", \"ì „í†µ\", \"ì²´í—˜\"],\n",
    "            \"ì‹œê°„\": \"12:00â€“15:00 / 18:00â€“21:00\",\n",
    "            \"ë©”ë‰´\": [\"ì „í†µ ê³±ì°½ì „ê³¨\", \"í•œêµ­ìˆ \"],\n",
    "            \"ë¯¼ê°ë„\": \"ë‚®ìŒ\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    def strategy(self, district: str, district_type: str) -> str:\n",
    "        if district_type == \"í”„ë¦¬ë¯¸ì—„ ë¹„ì¦ˆë‹ˆìŠ¤\":\n",
    "            pri, sec = \"ì§ì¥ì¸ íšŒì‹ì¡±\", \"ì Šì€ì¸µ ë°ì´íŠ¸ì¡±\"\n",
    "        elif district_type == \"ëŒ€í•™ê°€\":\n",
    "            pri, sec = \"ì Šì€ì¸µ ë°ì´íŠ¸ì¡±\", \"ì§ì¥ì¸ íšŒì‹ì¡±\"\n",
    "        elif district_type == \"ê´€ê´‘/ì „í†µìƒê¶Œ\":\n",
    "            pri, sec = \"ê´€ê´‘ê°\", \"ê°€ì¡± ì™¸ì‹ì¡±\"\n",
    "        else:\n",
    "            pri, sec = \"ê°€ì¡± ì™¸ì‹ì¡±\", \"ì§ì¥ì¸ íšŒì‹ì¡±\"\n",
    "\n",
    "        def fmt(seg: str) -> str:\n",
    "            s = self.SEGMENTS[seg]\n",
    "            return (\n",
    "                f\"â— {seg}\\n\"\n",
    "                f\"  â”œ íŠ¹ì§•: {', '.join(s['íŠ¹ì§•'])}\\n\"\n",
    "                f\"  â”œ í”¼í¬íƒ€ì„: {s['ì‹œê°„']}\\n\"\n",
    "                f\"  â”œ ì¶”ì²œë©”ë‰´: {', '.join(s['ë©”ë‰´'])}\\n\"\n",
    "                f\"  â”” ê°€ê²©ë¯¼ê°ë„: {s['ë¯¼ê°ë„']}\"\n",
    "            )\n",
    "\n",
    "        return f\"ğŸ¯ {district} íƒ€ê²Ÿ ì „ëµ\\n{fmt(pri)}\\n\\në³´ì¡°\\n{fmt(sec)}\"\n",
    "\n",
    "# ========================\n",
    "# 5. ë¦¬í¬íŠ¸ & ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸\n",
    "# ========================\n",
    "\n",
    "def run_analysis():\n",
    "    \"\"\"ì—”ë“œ-íˆ¬-ì—”ë“œ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
    "\n",
    "    # â–¶ï¸ STEPÂ 0: API í‚¤ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ â†’ ì¸ì ì „ë‹¬)\n",
    "    client_id = os.getenv(\"NAVER_CLIENT_ID\", \"QjzwUZBwBgZijbaadyKV\")\n",
    "    client_secret = os.getenv(\"NAVER_CLIENT_SECRET\", \"fIqgUfyng8\")\n",
    "\n",
    "    client = NaverAPIClient(client_id, client_secret)\n",
    "    trend_analyzer = GopchangTrendAnalyzer(client)\n",
    "\n",
    "    # â–¶ï¸ STEPÂ 1: ë°ì´í„° ìˆ˜ì§‘ & ì „ì²˜ë¦¬\n",
    "    raw = trend_analyzer.collect(days=30)\n",
    "    df = trend_analyzer.preprocess(raw)\n",
    "\n",
    "    # â–¶ï¸ STEPÂ 2: ì£¼ìš” ìƒê¶Œ ì¸ì‚¬ì´íŠ¸ & íƒ€ê²ŸíŒ…\n",
    "    districts_info = [\n",
    "        (\"ê°•ë‚¨ì—­\", \"í”„ë¦¬ë¯¸ì—„ ë¹„ì¦ˆë‹ˆìŠ¤\"),\n",
    "        (\"ëª…ë™\", \"ê´€ê´‘/ì „í†µìƒê¶Œ\"),\n",
    "        (\"í™ëŒ€\", \"ëŒ€í•™ê°€\"),\n",
    "        (\"ì‹ ì´Œ\", \"ëŒ€í•™ê°€\"),\n",
    "        (\"ì¢…ë¡œ\", \"ê´€ê´‘/ì „í†µìƒê¶Œ\"),\n",
    "    ]\n",
    "\n",
    "    target_analyzer = GopchangTargetAnalyzer()\n",
    "\n",
    "    insights: List[str] = []\n",
    "    for d, d_type in districts_info:\n",
    "        insights.append(trend_analyzer.district_insight(df, d))\n",
    "        insights.append(target_analyzer.strategy(d, d_type))\n",
    "        insights.append(\"-\" * 40)\n",
    "\n",
    "    # â–¶ï¸ STEPÂ 3: ì›Œë“œí´ë¼ìš°ë“œ ì˜ˆì‹œ (ì œëª© ê¸°ì¤€)\n",
    "    wc_text = \" \".join(df[\"title\"].tolist())\n",
    "    WordCloud(\n",
    "        font_path=\"/System/Library/Fonts/AppleGothic.ttf\",\n",
    "        background_color=\"white\",\n",
    "        width=800,\n",
    "        height=400,\n",
    "    ).generate(wc_text).to_file(\"wordcloud_gopchang.png\")\n",
    "\n",
    "    # â–¶ï¸ STEPÂ 4: ì½˜ì†” ë¦¬í¬íŠ¸ ì¶œë ¥ & ì €ì¥\n",
    "    report = (\n",
    "        \"\\n\".join(insights)\n",
    "        + \"\\n\" + \"=\" * 60 + \"\\n\"\n",
    "        + f\"Generated: {datetime.now():%Y-%m-%d %H:%M}\"\n",
    "    )\n",
    "    print(report)\n",
    "    with open(\"gopchang_report.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(report)\n",
    "\n",
    "    print(\"âœ… ë¶„ì„ ì™„ë£Œ â€” 'gopchang_report.txt', 'wordcloud_gopchang.png' ìƒì„±\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ë„¤ì´ë²„ API ê³±ì°½ ë¶„ì„ ì½”ë“œ\n",
    "# API í‚¤ë§Œ ì…ë ¥í•˜ë©´ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥!\n",
    "# ================================================================================\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "class NaverGopchangAnalyzer:\n",
    "    \"\"\"ê³±ì°½ì§‘ ì°½ì—…ì„ ìœ„í•œ ë„¤ì´ë²„ API ë¶„ì„ê¸°\"\"\"\n",
    "    \n",
    "    def __init__(self, client_id, client_secret):\n",
    "        self.client_id = client_id\n",
    "        self.client_secret = client_secret\n",
    "        self.base_url = \"https://openapi.naver.com/v1/search\"\n",
    "        self.headers = {\n",
    "            'X-Naver-Client-Id': client_id,\n",
    "            'X-Naver-Client-Secret': client_secret\n",
    "        }\n",
    "        \n",
    "    def _api_call(self, endpoint, query, display=100, sort='date'):\n",
    "        \"\"\"API í˜¸ì¶œ ê³µí†µ í•¨ìˆ˜\"\"\"\n",
    "        url = f\"{self.base_url}/{endpoint}.json\"\n",
    "        params = {\n",
    "            'query': query,\n",
    "            'display': display,\n",
    "            'sort': sort\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, params=params)\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            else:\n",
    "                print(f\"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            return None\n",
    "\n",
    "    def analyze_district_popularity(self, districts=None):\n",
    "        \"\"\"ğŸ† ì§€ì—­ë³„ ê³±ì°½ ì¸ê¸°ë„ ë¶„ì„ - ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥!\"\"\"\n",
    "        \n",
    "        if districts is None:\n",
    "            districts = [\"ê°•ë‚¨ì—­\", \"í™ëŒ€\", \"ì‹ ì´Œ\", \"ëª…ë™\", \"ì¢…ë¡œ\", \"ì ì‹¤\", \"ê±´ëŒ€\", \"ì‹ ë¦¼\"]\n",
    "        \n",
    "        print(\"ğŸ” ì§€ì—­ë³„ ê³±ì°½ ì¸ê¸°ë„ ë¶„ì„ ì¤‘...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for district in districts:\n",
    "            print(f\"ğŸ“ {district} ë¶„ì„ ì¤‘...\", end=\" \")\n",
    "            \n",
    "            # ë¸”ë¡œê·¸ ê²€ìƒ‰\n",
    "            blog_data = self._api_call('blog', f\"{district} ê³±ì°½\", display=100)\n",
    "            blog_count = blog_data['total'] if blog_data else 0\n",
    "            \n",
    "            # ë‰´ìŠ¤ ê²€ìƒ‰  \n",
    "            news_data = self._api_call('news', f\"{district} ê³±ì°½\", display=50)\n",
    "            news_count = news_data['total'] if news_data else 0\n",
    "            \n",
    "            # ì¹´í˜ ê²€ìƒ‰\n",
    "            cafe_data = self._api_call('cafearticle', f\"{district} ê³±ì°½\", display=50)\n",
    "            cafe_count = cafe_data['total'] if cafe_data else 0\n",
    "            \n",
    "            total_mentions = blog_count + news_count + cafe_count\n",
    "            \n",
    "            results.append({\n",
    "                'district': district,\n",
    "                'blog_mentions': blog_count,\n",
    "                'news_mentions': news_count, \n",
    "                'cafe_mentions': cafe_count,\n",
    "                'total_mentions': total_mentions\n",
    "            })\n",
    "            \n",
    "            print(f\"âœ… ì´ {total_mentions:,}ê±´\")\n",
    "            time.sleep(0.1)  # API í˜¸ì¶œ ì œí•œ ê³ ë ¤\n",
    "        \n",
    "        # ê²°ê³¼ ì •ë ¬ ë° ì¶œë ¥\n",
    "        results.sort(key=lambda x: x['total_mentions'], reverse=True)\n",
    "        \n",
    "        print(f\"\\nğŸ† ì§€ì—­ë³„ ê³±ì°½ ì–¸ê¸‰ëŸ‰ ìˆœìœ„:\")\n",
    "        print(\"-\" * 60)\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"{i:2d}. {result['district']:<8} \"\n",
    "                  f\"ì´ {result['total_mentions']:>6,}ê±´ \"\n",
    "                  f\"(ë¸”ë¡œê·¸:{result['blog_mentions']:>4,} \"\n",
    "                  f\"ë‰´ìŠ¤:{result['news_mentions']:>3,} \"\n",
    "                  f\"ì¹´í˜:{result['cafe_mentions']:>3,})\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def analyze_competition_vs_other_foods(self):\n",
    "        \"\"\"ğŸ– ê³±ì°½ vs ë‹¤ë¥¸ ìŒì‹ ê²½ìŸë ¥ ë¶„ì„\"\"\"\n",
    "        \n",
    "        foods = {\n",
    "            \"ê³±ì°½\": [\"ê³±ì°½\", \"ë§‰ì°½\"],\n",
    "            \"ì¹˜í‚¨\": [\"ì¹˜í‚¨\", \"ë‹­ê°ˆë¹„\"], \n",
    "            \"ì‚¼ê²¹ì‚´\": [\"ì‚¼ê²¹ì‚´\", \"ê³ ê¸°êµ¬ì´\"],\n",
    "            \"ì¡±ë°œ\": [\"ì¡±ë°œ\", \"ë³´ìŒˆ\"],\n",
    "            \"ë§ˆë¼íƒ•\": [\"ë§ˆë¼íƒ•\", \"ë§ˆë¼ìƒ¹ê¶ˆ\"]\n",
    "        }\n",
    "        \n",
    "        print(\"\\nğŸ– ê³±ì°½ vs ë‹¤ë¥¸ ìŒì‹ ê²½ìŸë ¥ ë¶„ì„\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        competition_results = {}\n",
    "        \n",
    "        for food_category, keywords in foods.items():\n",
    "            total_mentions = 0\n",
    "            \n",
    "            for keyword in keywords:\n",
    "                blog_data = self._api_call('blog', keyword, display=100)\n",
    "                if blog_data:\n",
    "                    total_mentions += blog_data['total']\n",
    "                time.sleep(0.1)\n",
    "            \n",
    "            competition_results[food_category] = total_mentions\n",
    "            print(f\"ğŸ“Š {food_category:<6}: {total_mentions:>8,}ê±´\")\n",
    "        \n",
    "        # ê³±ì°½ì˜ ìƒëŒ€ì  ìœ„ì¹˜ ê³„ì‚°\n",
    "        gopchang_rank = sorted(competition_results.items(), \n",
    "                              key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nğŸ’¡ ë¶„ì„ ê²°ê³¼:\")\n",
    "        for i, (food, count) in enumerate(gopchang_rank, 1):\n",
    "            emoji = \"ğŸ¥‡\" if i == 1 else \"ğŸ¥ˆ\" if i == 2 else \"ğŸ¥‰\" if i == 3 else \"ğŸ“\"\n",
    "            print(f\"   {emoji} {i}ìœ„: {food} ({count:,}ê±´)\")\n",
    "        \n",
    "        return competition_results\n",
    "\n",
    "    def analyze_time_demand_patterns(self):\n",
    "        \"\"\"â° ì‹œê°„ëŒ€ë³„ ê³±ì°½ ìˆ˜ìš” íŒ¨í„´ ë¶„ì„\"\"\"\n",
    "        \n",
    "        time_patterns = {\n",
    "            \"ì ì‹¬\": \"ê³±ì°½ ì ì‹¬\",\n",
    "            \"ì €ë…\": \"ê³±ì°½ ì €ë…\", \n",
    "            \"ì•¼ì‹\": \"ê³±ì°½ ì•¼ì‹\",\n",
    "            \"ìƒˆë²½\": \"ê³±ì°½ ìƒˆë²½\",\n",
    "            \"ì£¼ë§\": \"ê³±ì°½ ì£¼ë§\",\n",
    "            \"í‰ì¼\": \"ê³±ì°½ í‰ì¼\",\n",
    "            \"ê¸ˆìš”ì¼\": \"ê³±ì°½ ê¸ˆìš”ì¼\"\n",
    "        }\n",
    "        \n",
    "        print(\"\\nâ° ì‹œê°„ëŒ€ë³„ ê³±ì°½ ìˆ˜ìš” íŒ¨í„´ ë¶„ì„\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        time_results = {}\n",
    "        \n",
    "        for time_type, keyword in time_patterns.items():\n",
    "            blog_data = self._api_call('blog', keyword, display=50)\n",
    "            mentions = blog_data['total'] if blog_data else 0\n",
    "            time_results[time_type] = mentions\n",
    "            \n",
    "            print(f\"ğŸ• {time_type:<4}: {mentions:>5,}ê±´\")\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        # ìµœê³  ìˆ˜ìš” ì‹œê°„ëŒ€ ì°¾ê¸°\n",
    "        peak_time = max(time_results.items(), key=lambda x: x[1])\n",
    "        print(f\"\\nğŸ”¥ ìµœê³  ìˆ˜ìš”: {peak_time[0]} ({peak_time[1]:,}ê±´)\")\n",
    "        \n",
    "        return time_results\n",
    "\n",
    "    def analyze_price_sensitivity(self):\n",
    "        \"\"\"ğŸ’° ê³±ì°½ ê°€ê²© ë¯¼ê°ë„ ë¶„ì„\"\"\"\n",
    "        \n",
    "        price_keywords = [\n",
    "            \"ê³±ì°½ 1ë§Œì›\", \"ê³±ì°½ 2ë§Œì›\", \"ê³±ì°½ 3ë§Œì›\", \"ê³±ì°½ 4ë§Œì›\",\n",
    "            \"ê³±ì°½ ë¹„ì‹¸ë‹¤\", \"ê³±ì°½ ì €ë ´\", \"ê³±ì°½ ê°€ì„±ë¹„\", \"ê³±ì°½ ê°€ê²©\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nğŸ’° ê³±ì°½ ê°€ê²© ë¯¼ê°ë„ ë¶„ì„\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        price_results = {}\n",
    "        \n",
    "        for keyword in price_keywords:\n",
    "            blog_data = self._api_call('blog', keyword, display=30)\n",
    "            mentions = blog_data['total'] if blog_data else 0\n",
    "            price_results[keyword] = mentions\n",
    "            \n",
    "            print(f\"ğŸ’µ {keyword:<12}: {mentions:>4,}ê±´\")\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        return price_results\n",
    "\n",
    "    def analyze_customer_complaints(self, district=\"ê°•ë‚¨\"):\n",
    "        \"\"\"ğŸ˜¤ ê³ ê° ë¶ˆë§Œì‚¬í•­ ë¶„ì„\"\"\"\n",
    "        \n",
    "        complaint_keywords = [\n",
    "            f\"{district} ê³±ì°½ ë³„ë¡œ\",\n",
    "            f\"{district} ê³±ì°½ ì‹¤ë§\", \n",
    "            f\"{district} ê³±ì°½ ë¹„ì‹¸\",\n",
    "            f\"{district} ê³±ì°½ ë¶ˆì¹œì ˆ\",\n",
    "            f\"{district} ê³±ì°½ ë”ëŸ½\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nğŸ˜¤ {district} ì§€ì—­ ê³±ì°½ì§‘ ê³ ê° ë¶ˆë§Œ ë¶„ì„\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        complaints = {}\n",
    "        total_complaints = 0\n",
    "        \n",
    "        for keyword in complaint_keywords:\n",
    "            blog_data = self._api_call('blog', keyword, display=20)\n",
    "            count = blog_data['total'] if blog_data else 0\n",
    "            complaints[keyword] = count\n",
    "            total_complaints += count\n",
    "            \n",
    "            if count > 0:\n",
    "                issue_type = keyword.split()[-1]\n",
    "                print(f\"âš ï¸  {issue_type:<6}: {count:>3,}ê±´\")\n",
    "            \n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        if total_complaints == 0:\n",
    "            print(\"âœ… íŠ¹ë³„í•œ ë¶ˆë§Œì‚¬í•­ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!\")\n",
    "        else:\n",
    "            print(f\"\\nğŸ“Š ì´ ë¶ˆë§Œ ì–¸ê¸‰: {total_complaints}ê±´\")\n",
    "        \n",
    "        return complaints\n",
    "\n",
    "    def find_trending_keywords(self):\n",
    "        \"\"\"ğŸ”¥ ê³±ì°½ ê´€ë ¨ íŠ¸ë Œë”© í‚¤ì›Œë“œ ë°œêµ´\"\"\"\n",
    "        \n",
    "        trending_concepts = [\n",
    "            \"ê³±ì°½ ë‹¤ì´ì–´íŠ¸\", \"ê³±ì°½ ê±´ê°•\", \"ê³±ì°½ ASMR\",\n",
    "            \"ê³±ì°½ ë¨¹ë°©\", \"ê³±ì°½ ì¸ìŠ¤íƒ€\", \"ê³±ì°½ MZì„¸ëŒ€\",\n",
    "            \"ê³±ì°½ ë°°ë‹¬\", \"ê³±ì°½ í…Œì´í¬ì•„ì›ƒ\", \"ê³±ì°½ í™ˆíŒŒí‹°\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nğŸ”¥ ê³±ì°½ íŠ¸ë Œë”© í‚¤ì›Œë“œ ë¶„ì„\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        trend_scores = {}\n",
    "        \n",
    "        for keyword in trending_concepts:\n",
    "            blog_data = self._api_call('blog', keyword, display=30)\n",
    "            mentions = blog_data['total'] if blog_data else 0\n",
    "            \n",
    "            # 2024ë…„ ì–¸ê¸‰ëŸ‰ìœ¼ë¡œ íŠ¸ë Œë“œ ì ìˆ˜ ê³„ì‚°\n",
    "            recent_data = self._api_call('blog', f\"{keyword} 2024\", display=10)\n",
    "            recent_mentions = recent_data['total'] if recent_data else 0\n",
    "            \n",
    "            # íŠ¸ë Œë“œ ì ìˆ˜ = ìµœê·¼ ì–¸ê¸‰ ë¹„ìœ¨\n",
    "            trend_score = (recent_mentions / max(mentions, 1)) * 100\n",
    "            trend_scores[keyword] = {\n",
    "                'total': mentions,\n",
    "                'recent': recent_mentions,\n",
    "                'trend_score': trend_score\n",
    "            }\n",
    "            \n",
    "            trend_emoji = \"ğŸš€\" if trend_score > 50 else \"ğŸ“ˆ\" if trend_score > 20 else \"ğŸ“Š\"\n",
    "            print(f\"{trend_emoji} {keyword:<12}: {mentions:>3,}ê±´ (íŠ¸ë Œë“œ:{trend_score:>4.1f}%)\")\n",
    "            \n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        return trend_scores\n",
    "\n",
    "    def generate_location_insight(self, target_district):\n",
    "        \"\"\"ğŸ¯ íŠ¹ì • ì§€ì—­ ì‹¬ì¸µ ë¶„ì„ ë¦¬í¬íŠ¸\"\"\"\n",
    "        \n",
    "        print(f\"\\nğŸ¯ {target_district} ê³±ì°½ ì‹œì¥ ì‹¬ì¸µ ë¶„ì„\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. ì „ì²´ ì‹œì¥ ê·œëª¨\n",
    "        market_data = self._api_call('blog', f\"{target_district} ê³±ì°½\", display=100)\n",
    "        market_size = market_data['total'] if market_data else 0\n",
    "        \n",
    "        # 2. ê²½ìŸì—…ì²´ ì¡°ì‚¬\n",
    "        competitors = self._api_call('local', f\"{target_district} ê³±ì°½ì§‘\", display=5)\n",
    "        competitor_count = len(competitors['items']) if competitors else 0\n",
    "        \n",
    "        # 3. ê³ ê° ë§Œì¡±ë„\n",
    "        positive_data = self._api_call('blog', f\"{target_district} ê³±ì°½ ë§›ìˆë‹¤\", display=30)\n",
    "        positive_mentions = positive_data['total'] if positive_data else 0\n",
    "        \n",
    "        # 4. ê°€ê²© ê´€ë ¨ ì–¸ê¸‰\n",
    "        price_data = self._api_call('blog', f\"{target_district} ê³±ì°½ ê°€ê²©\", display=30)\n",
    "        price_mentions = price_data['total'] if price_data else 0\n",
    "        \n",
    "        # ê²°ê³¼ ì¶œë ¥\n",
    "        print(f\"ğŸ“Š ì‹œì¥ ê·œëª¨: {market_size:,}ê°œ ì˜¨ë¼ì¸ ì–¸ê¸‰\")\n",
    "        print(f\"ğŸª ê²½ìŸì—…ì²´: {competitor_count}ê°œ ê³±ì°½ì§‘ ë°œê²¬\")\n",
    "        print(f\"ğŸ˜Š ê¸ì • í‰ê°€: {positive_mentions:,}ê±´\")\n",
    "        print(f\"ğŸ’° ê°€ê²© ë…¼ì˜: {price_mentions:,}ê±´\")\n",
    "        \n",
    "        # ì‹œì¥ ê¸°íšŒ ì ìˆ˜ ê³„ì‚° (ê°„ë‹¨í•œ ê³µì‹)\n",
    "        opportunity_score = min(100, \n",
    "            (market_size * 0.4) + \n",
    "            (max(0, 10 - competitor_count) * 5) + \n",
    "            (positive_mentions * 0.3)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nğŸ¯ ì‹œì¥ ê¸°íšŒ ì ìˆ˜: {opportunity_score:.1f}/100ì \")\n",
    "        \n",
    "        if opportunity_score >= 70:\n",
    "            print(\"âœ… ë§¤ìš° ìœ ë§í•œ ì§€ì—­ì…ë‹ˆë‹¤!\")\n",
    "        elif opportunity_score >= 50:\n",
    "            print(\"âš ï¸  ì‹ ì¤‘í•œ ê²€í† ê°€ í•„ìš”í•œ ì§€ì—­ì…ë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            print(\"âŒ ì§„ì…ì´ ì–´ë ¤ìš´ ì§€ì—­ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\")\n",
    "        \n",
    "        return {\n",
    "            'market_size': market_size,\n",
    "            'competitors': competitor_count,\n",
    "            'positive_mentions': positive_mentions,\n",
    "            'opportunity_score': opportunity_score\n",
    "        }\n",
    "\n",
    "    def run_comprehensive_analysis(self, target_district=\"ê°•ë‚¨ì—­\"):\n",
    "        \"\"\"ğŸš€ ì¢…í•© ë¶„ì„ ì‹¤í–‰\"\"\"\n",
    "        \n",
    "        print(\"ğŸ– ë„¤ì´ë²„ API ê³±ì°½ì§‘ ì°½ì—… ë¶„ì„ ì‹œì‘!\")\n",
    "        print(f\"ğŸ“… ë¶„ì„ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # ì „ì²´ ë¶„ì„ ì‹¤í–‰\n",
    "        try:\n",
    "            # 1. ì§€ì—­ë³„ ì¸ê¸°ë„\n",
    "            regional_results = self.analyze_district_popularity()\n",
    "            \n",
    "            # 2. ìŒì‹ ê²½ìŸë ¥\n",
    "            food_competition = self.analyze_competition_vs_other_foods()\n",
    "            \n",
    "            # 3. ì‹œê°„ëŒ€ë³„ ìˆ˜ìš”\n",
    "            time_patterns = self.analyze_time_demand_patterns()\n",
    "            \n",
    "            # 4. ê°€ê²© ë¯¼ê°ë„\n",
    "            price_analysis = self.analyze_price_sensitivity()\n",
    "            \n",
    "            # 5. íŠ¸ë Œë”© í‚¤ì›Œë“œ\n",
    "            trending = self.find_trending_keywords()\n",
    "            \n",
    "            # 6. íƒ€ê²Ÿ ì§€ì—­ ì‹¬ì¸µ ë¶„ì„\n",
    "            location_insight = self.generate_location_insight(target_district)\n",
    "            \n",
    "            # 7. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±\n",
    "            self._generate_final_report(regional_results, food_competition, \n",
    "                                      time_patterns, location_insight, target_district)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            print(\"ğŸ’¡ API í‚¤ì™€ ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "    def _generate_final_report(self, regional_results, food_competition, \n",
    "                              time_patterns, location_insight, target_district):\n",
    "        \"\"\"ğŸ“‹ ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±\"\"\"\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ {target_district} ê³±ì°½ì§‘ ì°½ì—… ë¶„ì„ ìµœì¢… ë¦¬í¬íŠ¸\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # TOP 3 ì¶”ì²œ ì§€ì—­\n",
    "        top_regions = regional_results[:3]\n",
    "        print(f\"ğŸ† ì¶”ì²œ ì°½ì—… ì§€ì—­ TOP 3:\")\n",
    "        for i, region in enumerate(top_regions, 1):\n",
    "            print(f\"   {i}. {region['district']} - {region['total_mentions']:,}ê±´\")\n",
    "        \n",
    "        # ê³±ì°½ ì‹œì¥ í¬ì§€ì…˜\n",
    "        gopchang_rank = list(food_competition.keys()).index('ê³±ì°½') + 1\n",
    "        print(f\"\\nğŸ“Š ê³±ì°½ ì‹œì¥ í¬ì§€ì…˜: ìŒì‹ì—…ê³„ {gopchang_rank}ìœ„\")\n",
    "        \n",
    "        # ìµœì  ìš´ì˜ ì‹œê°„\n",
    "        peak_time = max(time_patterns.items(), key=lambda x: x[1])\n",
    "        print(f\"â° ìµœì  ìš´ì˜ ì‹œê°„: {peak_time[0]} (ìˆ˜ìš” {peak_time[1]:,}ê±´)\")\n",
    "        \n",
    "        # íƒ€ê²Ÿ ì§€ì—­ í‰ê°€\n",
    "        opportunity = location_insight['opportunity_score']\n",
    "        print(f\"ğŸ¯ {target_district} ì°½ì—… ì í•©ë„: {opportunity:.1f}ì \")\n",
    "        \n",
    "        # í•µì‹¬ ì œì•ˆì‚¬í•­\n",
    "        print(f\"\\nğŸ’¡ í•µì‹¬ ì œì•ˆì‚¬í•­:\")\n",
    "        if opportunity >= 70:\n",
    "            print(f\"   âœ… {target_district}ëŠ” ê³±ì°½ì§‘ ì°½ì—…ì— ì í•©í•œ ì§€ì—­ì…ë‹ˆë‹¤\")\n",
    "            print(f\"   âœ… {peak_time[0]} ì‹œê°„ëŒ€ íŠ¹í™” ìš´ì˜ ê¶Œì¥\")\n",
    "        else:\n",
    "            alternatives = [r['district'] for r in top_regions if r['district'] != target_district][:2]\n",
    "            print(f\"   âš ï¸  {target_district} ëŒ€ì‹  {', '.join(alternatives)} ì§€ì—­ ê³ ë ¤\")\n",
    "        \n",
    "        print(f\"   ğŸ“ˆ ì˜¨ë¼ì¸ ë§ˆì¼€íŒ… í•„ìˆ˜ (ë†’ì€ ì˜¨ë¼ì¸ ê´€ì‹¬ë„)\")\n",
    "        print(f\"   ğŸª ì°¨ë³„í™”ëœ ì»¨ì…‰ìœ¼ë¡œ ê²½ìŸë ¥ í™•ë³´ í•„ìš”\")\n",
    "\n",
    "# ğŸš€ ì‹¤í–‰ í•¨ìˆ˜\n",
    "def main():\n",
    "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”‘ ë„¤ì´ë²„ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”:\")\n",
    "    print(\"(ë„¤ì´ë²„ ê°œë°œìì„¼í„°ì—ì„œ ë°œê¸‰ë°›ìœ¼ì„¸ìš”: https://developers.naver.com/)\")\n",
    "    \n",
    "    # API í‚¤ ì…ë ¥ (ì‹¤ì œ ì‚¬ìš© ì‹œ ì—¬ê¸°ì— ì…ë ¥)\n",
    "    CLIENT_ID = input(\"Client ID: \").strip()\n",
    "    CLIENT_SECRET = input(\"Client Secret: \").strip()\n",
    "    \n",
    "    if not CLIENT_ID or not CLIENT_SECRET:\n",
    "        print(\"âŒ API í‚¤ê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"\\nğŸ’¡ í…ŒìŠ¤íŠ¸ë¥¼ ì›í•˜ì‹œë©´ ë‹¤ìŒê³¼ ê°™ì´ ì§ì ‘ ì…ë ¥í•˜ì„¸ìš”:\")\n",
    "        print('analyzer = NaverGopchangAnalyzer(\"your_id\", \"your_secret\")')\n",
    "        print('analyzer.run_comprehensive_analysis(\"ì›í•˜ëŠ”ì§€ì—­\")')\n",
    "        return\n",
    "    \n",
    "    # ë¶„ì„ ì‹¤í–‰\n",
    "    target_area = input(\"ë¶„ì„í•  ì§€ì—­ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ê°•ë‚¨ì—­): \").strip() or \"ê°•ë‚¨ì—­\"\n",
    "    \n",
    "    analyzer = NaverGopchangAnalyzer(CLIENT_ID, CLIENT_SECRET)\n",
    "    analyzer.run_comprehensive_analysis(target_area)\n",
    "    \n",
    "    print(f\"\\nâœ… ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ í˜„ëª…í•œ ì°½ì—… ê²°ì •í•˜ì„¸ìš”! ğŸ–\")\n",
    "\n",
    "# ê°œë³„ ë¶„ì„ í•¨ìˆ˜ë“¤ (ì›í•˜ëŠ” ë¶„ì„ë§Œ ì‹¤í–‰ ê°€ëŠ¥)\n",
    "def quick_popularity_check(client_id, client_secret):\n",
    "    \"\"\"ë¹ ë¥¸ ì¸ê¸°ë„ ì²´í¬ (1ë¶„)\"\"\"\n",
    "    analyzer = NaverGopchangAnalyzer(client_id, client_secret)\n",
    "    return analyzer.analyze_district_popularity()\n",
    "\n",
    "def quick_competition_check(client_id, client_secret):\n",
    "    \"\"\"ë¹ ë¥¸ ê²½ìŸ í˜„í™© ì²´í¬ (30ì´ˆ)\"\"\"\n",
    "    analyzer = NaverGopchangAnalyzer(client_id, client_secret)\n",
    "    return analyzer.analyze_competition_vs_other_foods()\n",
    "\n",
    "def quick_location_check(client_id, client_secret, location):\n",
    "    \"\"\"íŠ¹ì • ì§€ì—­ ë¹ ë¥¸ ì²´í¬ (30ì´ˆ)\"\"\"\n",
    "    analyzer = NaverGopchangAnalyzer(client_id, client_secret)\n",
    "    return analyzer.generate_location_insight(location)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "    # ë˜ëŠ” ê°œë³„ ì‹¤í–‰ ì˜ˆì‹œ:\n",
    "    # analyzer = NaverGopchangAnalyzer(\"your_id\", \"your_secret\")\n",
    "    # analyzer.analyze_district_popularity()  # ì§€ì—­ë³„ ì¸ê¸°ë„ë§Œ\n",
    "    # analyzer.analyze_time_demand_patterns()  # ì‹œê°„ëŒ€ë³„ ë¶„ì„ë§Œ\n",
    "    # analyzer.generate_location_insight(\"í™ëŒ€\")  # í™ëŒ€ ë¶„ì„ë§Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” í™ëŒ€/ì ì‹¤/ì¢…ë¡œ TOP 3 ì§€ì—­ ì‹¬ì¸µ ë¶„ì„\n",
    "# ê° ì§€ì—­ë³„ ìƒì„¸ íŠ¹ì„± ë° ì°½ì—… ê¸°íšŒ ë¶„ì„\n",
    "# ================================================================================\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class DetailedRegionAnalyzer:\n",
    "    \"\"\"TOP 3 ì§€ì—­ ì‹¬ì¸µ ë¶„ì„ê¸°\"\"\"\n",
    "    \n",
    "    def __init__(self, client_id, client_secret):\n",
    "        self.client_id = client_id\n",
    "        self.client_secret = client_secret\n",
    "        self.headers = {\n",
    "            'X-Naver-Client-Id': client_id,\n",
    "            'X-Naver-Client-Secret': client_secret\n",
    "        }\n",
    "        \n",
    "    def search_api(self, search_type, query, display=50):\n",
    "        \"\"\"ë„¤ì´ë²„ API ê²€ìƒ‰\"\"\"\n",
    "        url = f\"https://openapi.naver.com/v1/search/{search_type}.json\"\n",
    "        params = {'query': query, 'display': display, 'sort': 'date'}\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, params=params)\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            else:\n",
    "                return None\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def analyze_customer_characteristics(self, region):\n",
    "        \"\"\"ğŸ¯ ì§€ì—­ë³„ ê³ ê° íŠ¹ì„± ë¶„ì„\"\"\"\n",
    "        \n",
    "        customer_keywords = {\n",
    "            \"ëŒ€í•™ìƒ\": f\"{region} ê³±ì°½ ëŒ€í•™ìƒ\",\n",
    "            \"ì§ì¥ì¸\": f\"{region} ê³±ì°½ íšŒì‹\", \n",
    "            \"ê°€ì¡±\": f\"{region} ê³±ì°½ ê°€ì¡±\",\n",
    "            \"ì»¤í”Œ\": f\"{region} ê³±ì°½ ë°ì´íŠ¸\",\n",
    "            \"ê´€ê´‘ê°\": f\"{region} ê³±ì°½ ê´€ê´‘\",\n",
    "            \"ì™¸êµ­ì¸\": f\"{region} ê³±ì°½ ì™¸êµ­ì¸\"\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nğŸ¯ {region} ê³ ê° íŠ¹ì„± ë¶„ì„\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        customer_data = {}\n",
    "        total_customer_mentions = 0\n",
    "        \n",
    "        for customer_type, keyword in customer_keywords.items():\n",
    "            data = self.search_api('blog', keyword, display=30)\n",
    "            mentions = data['total'] if data else 0\n",
    "            customer_data[customer_type] = mentions\n",
    "            total_customer_mentions += mentions\n",
    "            \n",
    "            if mentions > 0:\n",
    "                percentage = (mentions / max(total_customer_mentions, 1)) * 100\n",
    "                bar = \"â–ˆ\" * min(15, mentions // 10)\n",
    "                print(f\"ğŸ‘¥ {customer_type:<6}: {mentions:>4,}ê±´ {bar}\")\n",
    "            \n",
    "            time.sleep(0.2)\n",
    "        \n",
    "        # ì£¼ìš” ê³ ê°ì¸µ ì°¾ê¸°\n",
    "        if total_customer_mentions > 0:\n",
    "            main_customers = sorted(customer_data.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "            print(f\"\\nğŸ¯ ì£¼ìš” ê³ ê°ì¸µ:\")\n",
    "            for i, (customer_type, count) in enumerate(main_customers, 1):\n",
    "                if count > 0:\n",
    "                    print(f\"   {i}. {customer_type} ({count:,}ê±´)\")\n",
    "        \n",
    "        return customer_data\n",
    "\n",
    "    def analyze_competition_intensity(self, region):\n",
    "        \"\"\"ğŸª ê²½ìŸ ê°•ë„ ë¶„ì„\"\"\"\n",
    "        \n",
    "        competition_keywords = [\n",
    "            f\"{region} ê³±ì°½ì§‘ ë§ë‹¤\",\n",
    "            f\"{region} ê³±ì°½ ë§›ì§‘\", \n",
    "            f\"{region} ê³±ì°½ ì¶”ì²œ\",\n",
    "            f\"{region} ê³±ì°½ì§‘ ê²½ìŸ\",\n",
    "            f\"{region} ê³±ì°½ í¬í™”\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nğŸª {region} ê²½ìŸ í™˜ê²½ ë¶„ì„\")\n",
    "        print(\"=\" * 25)\n",
    "        \n",
    "        competition_scores = {}\n",
    "        \n",
    "        # ì§€ì—­ ë‚´ ê³±ì°½ì§‘ ê²€ìƒ‰\n",
    "        local_data = self.search_api('local', f\"{region} ê³±ì°½ì§‘\", display=20)\n",
    "        if local_data and 'items' in local_data:\n",
    "            restaurant_count = len(local_data['items'])\n",
    "            print(f\"ğŸª ë“±ë¡ëœ ê³±ì°½ì§‘: {restaurant_count}ê°œ\")\n",
    "            \n",
    "            # ìƒìœ„ ê³±ì°½ì§‘ë“¤ì˜ ì˜¨ë¼ì¸ ì–¸ê¸‰ëŸ‰\n",
    "            if restaurant_count > 0:\n",
    "                print(f\"ğŸ“Š ì£¼ìš” ê²½ìŸì—…ì²´:\")\n",
    "                for i, restaurant in enumerate(local_data['items'][:5], 1):\n",
    "                    name = restaurant.get('title', '').replace('<b>', '').replace('</b>', '')\n",
    "                    # ì—…ì²´ëª…ìœ¼ë¡œ ê²€ìƒ‰ (ê°„ë‹¨íˆ)\n",
    "                    mentions = self.search_api('blog', name, display=10)\n",
    "                    mention_count = mentions['total'] if mentions else 0\n",
    "                    print(f\"   {i}. {name[:15]}: {mention_count:,}ê±´\")\n",
    "                    time.sleep(0.2)\n",
    "        else:\n",
    "            restaurant_count = 0\n",
    "            print(f\"ğŸª ë“±ë¡ëœ ê³±ì°½ì§‘: ê²€ìƒ‰ë˜ì§€ ì•ŠìŒ\")\n",
    "        \n",
    "        # ê²½ìŸ ê´€ë ¨ í‚¤ì›Œë“œ ë¶„ì„\n",
    "        for keyword in competition_keywords:\n",
    "            data = self.search_api('blog', keyword, display=20)\n",
    "            mentions = data['total'] if data else 0\n",
    "            competition_scores[keyword] = mentions\n",
    "            time.sleep(0.2)\n",
    "        \n",
    "        # ê²½ìŸ ê°•ë„ ì ìˆ˜ ê³„ì‚° (0-100)\n",
    "        competition_intensity = min(100, restaurant_count * 5)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ê²½ìŸ ê°•ë„: {competition_intensity}ì /100ì \")\n",
    "        if competition_intensity >= 70:\n",
    "            print(\"âš ï¸ ê²½ìŸ ë§¤ìš° ì¹˜ì—´\")\n",
    "        elif competition_intensity >= 40:\n",
    "            print(\"ğŸ“Š ì ì ˆí•œ ê²½ìŸ\")\n",
    "        else:\n",
    "            print(\"âœ… ê²½ìŸ ì—¬ìœ ë¡œì›€\")\n",
    "            \n",
    "        return {\n",
    "            'restaurant_count': restaurant_count,\n",
    "            'competition_intensity': competition_intensity,\n",
    "            'competition_scores': competition_scores\n",
    "        }\n",
    "\n",
    "    def analyze_price_trends(self, region):\n",
    "        \"\"\"ğŸ’° ì§€ì—­ë³„ ê°€ê²© íŠ¸ë Œë“œ ë¶„ì„\"\"\"\n",
    "        \n",
    "        price_keywords = {\n",
    "            \"ì €ê°€\": f\"{region} ê³±ì°½ ì €ë ´\",\n",
    "            \"ì¤‘ê°€\": f\"{region} ê³±ì°½ 2ë§Œì›\",\n",
    "            \"ê³ ê°€\": f\"{region} ê³±ì°½ ë¹„ì‹¸ë‹¤\",\n",
    "            \"ê°€ì„±ë¹„\": f\"{region} ê³±ì°½ ê°€ì„±ë¹„\"\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nğŸ’° {region} ê°€ê²© íŠ¸ë Œë“œ ë¶„ì„\")\n",
    "        print(\"=\" * 25)\n",
    "        \n",
    "        price_data = {}\n",
    "        total_price_mentions = 0\n",
    "        \n",
    "        for price_type, keyword in price_keywords.items():\n",
    "            data = self.search_api('blog', keyword, display=30)\n",
    "            mentions = data['total'] if data else 0\n",
    "            price_data[price_type] = mentions\n",
    "            total_price_mentions += mentions\n",
    "            \n",
    "            if mentions > 0:\n",
    "                percentage = (mentions / max(total_price_mentions, 1)) * 100\n",
    "                print(f\"ğŸ’µ {price_type:<4}: {mentions:>4,}ê±´ ({percentage:4.1f}%)\")\n",
    "            \n",
    "            time.sleep(0.2)\n",
    "        \n",
    "        # ê°€ê²© í¬ì§€ì…”ë‹ ì¶”ì²œ\n",
    "        if total_price_mentions > 0:\n",
    "            sorted_prices = sorted(price_data.items(), key=lambda x: x[1], reverse=True)\n",
    "            dominant_price = sorted_prices[0]\n",
    "            \n",
    "            print(f\"\\nğŸ¯ ì£¼ìš” ê°€ê²©ëŒ€: {dominant_price[0]} ({dominant_price[1]:,}ê±´)\")\n",
    "            \n",
    "            if dominant_price[0] == \"ê°€ì„±ë¹„\":\n",
    "                recommendation = \"ê°€ì„±ë¹„ ì¤‘ì‹¬ ì „ëµ ê¶Œì¥\"\n",
    "            elif dominant_price[0] == \"ì €ê°€\":\n",
    "                recommendation = \"ì €ê°€ê²© ê²½ìŸ ì£¼ì˜ í•„ìš”\"\n",
    "            elif dominant_price[0] == \"ê³ ê°€\":\n",
    "                recommendation = \"í”„ë¦¬ë¯¸ì—„ ì‹œì¥ ê¸°íšŒ\"\n",
    "            else:\n",
    "                recommendation = \"ì¤‘ê°„ ê°€ê²©ëŒ€ ì•ˆì •ì \"\n",
    "                \n",
    "            print(f\"ğŸ’¡ ì¶”ì²œ ì „ëµ: {recommendation}\")\n",
    "        \n",
    "        return price_data\n",
    "\n",
    "    def analyze_unique_opportunities(self, region):\n",
    "        \"\"\"ğŸ” ì§€ì—­ë³„ ê³ ìœ  ê¸°íšŒ ë¶„ì„\"\"\"\n",
    "        \n",
    "        # ì§€ì—­ë³„ íŠ¹í™” í‚¤ì›Œë“œ\n",
    "        if \"í™ëŒ€\" in region:\n",
    "            opportunity_keywords = [\n",
    "                f\"{region} ê³±ì°½ í´ëŸ½\",\n",
    "                f\"{region} ê³±ì°½ íœíŠ¸í•˜ìš°ìŠ¤\",\n",
    "                f\"{region} ê³±ì°½ í™ìŠ¤í„°\",\n",
    "                f\"{region} ê³±ì°½ ì¸ìŠ¤íƒ€\",\n",
    "                f\"{region} ê³±ì°½ ì Šì€\",\n",
    "                f\"{region} ê³±ì°½ ëŠ¦ì€ì‹œê°„\"\n",
    "            ]\n",
    "        elif \"ì ì‹¤\" in region:\n",
    "            opportunity_keywords = [\n",
    "                f\"{region} ê³±ì°½ ê°€ì¡±\",\n",
    "                f\"{region} ê³±ì°½ ë¡¯ë°ì›”ë“œ\",\n",
    "                f\"{region} ê³±ì°½ ì„ì´Œí˜¸ìˆ˜\",\n",
    "                f\"{region} ê³±ì°½ ì•„ì´\",\n",
    "                f\"{region} ê³±ì°½ ì£¼ë§\",\n",
    "                f\"{region} ê³±ì°½ ë„“ì€\"\n",
    "            ]\n",
    "        elif \"ì¢…ë¡œ\" in region:\n",
    "            opportunity_keywords = [\n",
    "                f\"{region} ê³±ì°½ ì „í†µ\",\n",
    "                f\"{region} ê³±ì°½ ê´€ê´‘\",\n",
    "                f\"{region} ê³±ì°½ ì™¸êµ­ì¸\",\n",
    "                f\"{region} ê³±ì°½ í•œêµ­\",\n",
    "                f\"{region} ê³±ì°½ ì—­ì‚¬\",\n",
    "                f\"{region} ê³±ì°½ ë¬¸í™”\"\n",
    "            ]\n",
    "        else:\n",
    "            opportunity_keywords = [f\"{region} ê³±ì°½ íŠ¹ë³„\"]\n",
    "        \n",
    "        print(f\"\\nğŸ” {region} ê³ ìœ  ê¸°íšŒ ë¶„ì„\")\n",
    "        print(\"=\" * 25)\n",
    "        \n",
    "        opportunities = {}\n",
    "        \n",
    "        for keyword in opportunity_keywords:\n",
    "            data = self.search_api('blog', keyword, display=20)\n",
    "            mentions = data['total'] if data else 0\n",
    "            if mentions > 0:\n",
    "                opportunities[keyword] = mentions\n",
    "                clean_keyword = keyword.replace(f\"{region} ê³±ì°½ \", \"\")\n",
    "                print(f\"ğŸ’¡ {clean_keyword:<8}: {mentions:>3,}ê±´\")\n",
    "            time.sleep(0.2)\n",
    "        \n",
    "        return opportunities\n",
    "\n",
    "    def analyze_time_patterns(self, region):\n",
    "        \"\"\"â° ì§€ì—­ë³„ ì‹œê°„ëŒ€ íŒ¨í„´ ë¶„ì„\"\"\"\n",
    "        \n",
    "        time_keywords = {\n",
    "            \"ì ì‹¬\": f\"{region} ê³±ì°½ ì ì‹¬\",\n",
    "            \"ì €ë…\": f\"{region} ê³±ì°½ ì €ë…\", \n",
    "            \"ì•¼ì‹\": f\"{region} ê³±ì°½ ì•¼ì‹\",\n",
    "            \"ìƒˆë²½\": f\"{region} ê³±ì°½ ìƒˆë²½\",\n",
    "            \"ì£¼ë§\": f\"{region} ê³±ì°½ ì£¼ë§\"\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nâ° {region} ì‹œê°„ëŒ€ë³„ ìˆ˜ìš”\")\n",
    "        print(\"=\" * 20)\n",
    "        \n",
    "        time_data = {}\n",
    "        \n",
    "        for time_type, keyword in time_keywords.items():\n",
    "            data = self.search_api('blog', keyword, display=30)\n",
    "            mentions = data['total'] if data else 0\n",
    "            time_data[time_type] = mentions\n",
    "            \n",
    "            if mentions > 0:\n",
    "                bar = \"â–ˆ\" * min(15, mentions // 50)\n",
    "                print(f\"ğŸ• {time_type:<4}: {mentions:>4,}ê±´ {bar}\")\n",
    "            \n",
    "            time.sleep(0.2)\n",
    "        \n",
    "        # ìµœì  ìš´ì˜ì‹œê°„ ì œì•ˆ\n",
    "        peak_time = max(time_data.items(), key=lambda x: x[1])\n",
    "        print(f\"\\nğŸ”¥ í”¼í¬ì‹œê°„: {peak_time[0]} ({peak_time[1]:,}ê±´)\")\n",
    "        \n",
    "        return time_data\n",
    "\n",
    "    def generate_region_swot(self, region, customer_data, competition_data, price_data, opportunities, time_data):\n",
    "        \"\"\"ğŸ“‹ ì§€ì—­ë³„ SWOT ë¶„ì„\"\"\"\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ {region} SWOT ë¶„ì„\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        # Strengths (ê°•ì )\n",
    "        print(\"ğŸ’ª Strengths (ê°•ì ):\")\n",
    "        \n",
    "        # ê³ ê° ë‹¤ì–‘ì„±\n",
    "        active_customers = [k for k, v in customer_data.items() if v > 100]\n",
    "        if len(active_customers) >= 3:\n",
    "            print(\"   âœ… ë‹¤ì–‘í•œ ê³ ê°ì¸µ í™•ë³´\")\n",
    "        \n",
    "        # ê²½ìŸ ì ì ˆì„±\n",
    "        if competition_data['competition_intensity'] < 70:\n",
    "            print(\"   âœ… ì ì ˆí•œ ê²½ìŸ í™˜ê²½\")\n",
    "        \n",
    "        # ê³ ìœ  ê¸°íšŒ\n",
    "        if opportunities:\n",
    "            top_opportunity = max(opportunities.items(), key=lambda x: x[1])\n",
    "            clean_opp = top_opportunity[0].replace(f\"{region} ê³±ì°½ \", \"\")\n",
    "            print(f\"   âœ… {clean_opp} íŠ¹í™” ê¸°íšŒ\")\n",
    "        \n",
    "        # Weaknesses (ì•½ì )\n",
    "        print(\"\\nâš ï¸ Weaknesses (ì•½ì ):\")\n",
    "        \n",
    "        if competition_data['competition_intensity'] >= 70:\n",
    "            print(\"   âŒ ë†’ì€ ê²½ìŸ ê°•ë„\")\n",
    "        \n",
    "        if competition_data['restaurant_count'] >= 15:\n",
    "            print(\"   âŒ ë§ì€ ê¸°ì¡´ ì—…ì²´\")\n",
    "        \n",
    "        # Opportunities (ê¸°íšŒ)\n",
    "        print(\"\\nğŸŒŸ Opportunities (ê¸°íšŒ):\")\n",
    "        \n",
    "        # ê°€ê²© ê¸°íšŒ\n",
    "        if price_data.get('ê°€ì„±ë¹„', 0) > price_data.get('ê³ ê°€', 0):\n",
    "            print(\"   ğŸ’¡ ê°€ì„±ë¹„ ì‹œì¥ ê³µëµ\")\n",
    "        \n",
    "        # ì‹œê°„ëŒ€ ê¸°íšŒ\n",
    "        peak_time = max(time_data.items(), key=lambda x: x[1])\n",
    "        print(f\"   ğŸ’¡ {peak_time[0]} ì‹œê°„ëŒ€ íŠ¹í™”\")\n",
    "        \n",
    "        # ê³ ìœ  ê¸°íšŒ\n",
    "        for keyword, mentions in sorted(opportunities.items(), key=lambda x: x[1], reverse=True)[:2]:\n",
    "            clean_keyword = keyword.replace(f\"{region} ê³±ì°½ \", \"\")\n",
    "            print(f\"   ğŸ’¡ {clean_keyword} ì»¨ì…‰ ê°œë°œ\")\n",
    "        \n",
    "        # Threats (ìœ„í˜‘)\n",
    "        print(\"\\nğŸš¨ Threats (ìœ„í˜‘):\")\n",
    "        \n",
    "        if competition_data['competition_intensity'] >= 50:\n",
    "            print(\"   âš¡ ê²½ìŸ ì‹¬í™”\")\n",
    "        \n",
    "        print(\"   âš¡ ì„ëŒ€ë£Œ ìƒìŠ¹ ê°€ëŠ¥ì„±\")\n",
    "        print(\"   âš¡ ê³ ê° ì·¨í–¥ ë³€í™”\")\n",
    "\n",
    "    def comprehensive_region_analysis(self, region):\n",
    "        \"\"\"ğŸ” ì§€ì—­ ì¢…í•© ë¶„ì„\"\"\"\n",
    "        \n",
    "        print(f\"\\nğŸ” {region} ì¢…í•© ì‹¬ì¸µ ë¶„ì„\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. ê³ ê° íŠ¹ì„± ë¶„ì„\n",
    "        customer_data = self.analyze_customer_characteristics(region)\n",
    "        \n",
    "        # 2. ê²½ìŸ í™˜ê²½ ë¶„ì„\n",
    "        competition_data = self.analyze_competition_intensity(region)\n",
    "        \n",
    "        # 3. ê°€ê²© íŠ¸ë Œë“œ ë¶„ì„\n",
    "        price_data = self.analyze_price_trends(region)\n",
    "        \n",
    "        # 4. ê³ ìœ  ê¸°íšŒ ë¶„ì„\n",
    "        opportunities = self.analyze_unique_opportunities(region)\n",
    "        \n",
    "        # 5. ì‹œê°„ëŒ€ íŒ¨í„´ ë¶„ì„\n",
    "        time_data = self.analyze_time_patterns(region)\n",
    "        \n",
    "        # 6. SWOT ë¶„ì„\n",
    "        self.generate_region_swot(region, customer_data, competition_data, price_data, opportunities, time_data)\n",
    "        \n",
    "        # 7. ì¢…í•© ì ìˆ˜ ê³„ì‚°\n",
    "        total_score = self.calculate_comprehensive_score(customer_data, competition_data, price_data, opportunities)\n",
    "        \n",
    "        print(f\"\\nğŸ¯ {region} ì¢…í•© ì ìˆ˜: {total_score:.1f}/100ì \")\n",
    "        \n",
    "        return {\n",
    "            'region': region,\n",
    "            'customer_data': customer_data,\n",
    "            'competition_data': competition_data,\n",
    "            'price_data': price_data,\n",
    "            'opportunities': opportunities,\n",
    "            'time_data': time_data,\n",
    "            'total_score': total_score\n",
    "        }\n",
    "    \n",
    "    def calculate_comprehensive_score(self, customer_data, competition_data, price_data, opportunities):\n",
    "        \"\"\"ì¢…í•© ì ìˆ˜ ê³„ì‚°\"\"\"\n",
    "        \n",
    "        # ê³ ê° ë‹¤ì–‘ì„± ì ìˆ˜ (0-30ì )\n",
    "        active_customers = len([v for v in customer_data.values() if v > 50])\n",
    "        customer_score = min(30, active_customers * 5)\n",
    "        \n",
    "        # ê²½ìŸ í™˜ê²½ ì ìˆ˜ (0-25ì ) - ê²½ìŸì´ ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜\n",
    "        competition_score = max(0, 25 - (competition_data['competition_intensity'] * 0.25))\n",
    "        \n",
    "        # ê°€ê²© ê¸°íšŒ ì ìˆ˜ (0-20ì )\n",
    "        price_score = min(20, sum(price_data.values()) * 0.001)\n",
    "        \n",
    "        # ê³ ìœ  ê¸°íšŒ ì ìˆ˜ (0-25ì )\n",
    "        opportunity_score = min(25, len(opportunities) * 5)\n",
    "        \n",
    "        total_score = customer_score + competition_score + price_score + opportunity_score\n",
    "        \n",
    "        return total_score\n",
    "\n",
    "    def compare_top3_regions(self):\n",
    "        \"\"\"ğŸ† TOP 3 ì§€ì—­ ë¹„êµ ë¶„ì„\"\"\"\n",
    "        \n",
    "        regions = [\"í™ëŒ€\", \"ì ì‹¤\", \"ì¢…ë¡œ\"]\n",
    "        \n",
    "        print(\"ğŸ† TOP 3 ì§€ì—­ ì‹¬ì¸µ ë¹„êµ ë¶„ì„\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ğŸ“… ë¶„ì„ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        region_results = {}\n",
    "        \n",
    "        for region in regions:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"ğŸ“ {region} ë¶„ì„ ì‹œì‘\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            result = self.comprehensive_region_analysis(region)\n",
    "            region_results[region] = result\n",
    "            \n",
    "            print(f\"\\nâœ… {region} ë¶„ì„ ì™„ë£Œ!\")\n",
    "            time.sleep(1)  # API ë¶€í•˜ ë°©ì§€\n",
    "        \n",
    "        # ìµœì¢… ë¹„êµ ë° ì¶”ì²œ\n",
    "        self.generate_final_comparison(region_results)\n",
    "        \n",
    "        return region_results\n",
    "    \n",
    "    def generate_final_comparison(self, region_results):\n",
    "        \"\"\"ğŸ“Š ìµœì¢… ë¹„êµ ë° ì¶”ì²œ\"\"\"\n",
    "        \n",
    "        print(f\"\\nğŸ“Š TOP 3 ì§€ì—­ ìµœì¢… ë¹„êµ\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # ì¢…í•© ì ìˆ˜ ìˆœìœ„\n",
    "        sorted_regions = sorted(region_results.items(), key=lambda x: x[1]['total_score'], reverse=True)\n",
    "        \n",
    "        print(f\"ğŸ† ì¢…í•© ì ìˆ˜ ìˆœìœ„:\")\n",
    "        for i, (region, data) in enumerate(sorted_regions, 1):\n",
    "            medal = \"ğŸ¥‡\" if i == 1 else \"ğŸ¥ˆ\" if i == 2 else \"ğŸ¥‰\"\n",
    "            print(f\"   {medal} {i}ìœ„: {region} ({data['total_score']:.1f}ì )\")\n",
    "        \n",
    "        # ì¹´í…Œê³ ë¦¬ë³„ ìµœê³ \n",
    "        print(f\"\\nğŸ¯ ì¹´í…Œê³ ë¦¬ë³„ ìµœê³  ì§€ì—­:\")\n",
    "        \n",
    "        # ê³ ê° ë‹¤ì–‘ì„±\n",
    "        customer_winner = max(region_results.items(), \n",
    "                            key=lambda x: len([v for v in x[1]['customer_data'].values() if v > 50]))\n",
    "        print(f\"   ğŸ‘¥ ê³ ê° ë‹¤ì–‘ì„±: {customer_winner[0]}\")\n",
    "        \n",
    "        # ê²½ìŸ ì—¬ìœ ë„\n",
    "        competition_winner = min(region_results.items(),\n",
    "                               key=lambda x: x[1]['competition_data']['competition_intensity'])\n",
    "        print(f\"   ğŸª ê²½ìŸ ì—¬ìœ ë„: {competition_winner[0]}\")\n",
    "        \n",
    "        # ê³ ìœ  ê¸°íšŒ\n",
    "        opportunity_winner = max(region_results.items(),\n",
    "                               key=lambda x: len(x[1]['opportunities']))\n",
    "        print(f\"   ğŸ’¡ ê³ ìœ  ê¸°íšŒ: {opportunity_winner[0]}\")\n",
    "        \n",
    "        # ìµœì¢… ì¶”ì²œ\n",
    "        winner = sorted_regions[0]\n",
    "        print(f\"\\nğŸ¯ ìµœì¢… ì¶”ì²œ: {winner[0]}\")\n",
    "        print(f\"ğŸ“Š ì¶”ì²œ ì´ìœ :\")\n",
    "        \n",
    "        winner_data = winner[1]\n",
    "        \n",
    "        # ê°•ì  ìš”ì•½\n",
    "        active_customers = len([v for v in winner_data['customer_data'].values() if v > 50])\n",
    "        print(f\"   âœ… {active_customers}ê°œ ê³ ê°ì¸µ í™•ë³´\")\n",
    "        print(f\"   âœ… ê²½ìŸê°•ë„ {winner_data['competition_data']['competition_intensity']}ì \")\n",
    "        print(f\"   âœ… {len(winner_data['opportunities'])}ê°œ ê³ ìœ  ê¸°íšŒ\")\n",
    "        \n",
    "        # êµ¬ì²´ì  ì „ëµ ì œì•ˆ\n",
    "        print(f\"\\nğŸ’¡ {winner[0]} ì°½ì—… ì „ëµ:\")\n",
    "        \n",
    "        # ì£¼ìš” ê³ ê°ì¸µ\n",
    "        main_customers = sorted(winner_data['customer_data'].items(), key=lambda x: x[1], reverse=True)[:2]\n",
    "        if main_customers[0][1] > 0:\n",
    "            print(f\"   ğŸ¯ ì£¼ìš” íƒ€ê²Ÿ: {main_customers[0][0]}\")\n",
    "        \n",
    "        # ìµœì  ì‹œê°„ëŒ€\n",
    "        peak_time = max(winner_data['time_data'].items(), key=lambda x: x[1])\n",
    "        print(f\"   â° ìµœì  ìš´ì˜: {peak_time[0]} íŠ¹í™”\")\n",
    "        \n",
    "        # ê°€ê²© ì „ëµ\n",
    "        main_price = max(winner_data['price_data'].items(), key=lambda x: x[1])\n",
    "        print(f\"   ğŸ’° ê°€ê²© ì „ëµ: {main_price[0]} ì¤‘ì‹¬\")\n",
    "        \n",
    "        # ì°¨ë³„í™” í¬ì¸íŠ¸\n",
    "        if winner_data['opportunities']:\n",
    "            top_opp = max(winner_data['opportunities'].items(), key=lambda x: x[1])\n",
    "            clean_opp = top_opp[0].replace(f\"{winner[0]} ê³±ì°½ \", \"\")\n",
    "            print(f\"   ğŸ”¥ ì°¨ë³„í™”: {clean_opp} ì»¨ì…‰\")\n",
    "\n",
    "# ğŸš€ ì‹¤í–‰ í•¨ìˆ˜\n",
    "def analyze_top3_regions(client_id, client_secret):\n",
    "    \"\"\"TOP 3 ì§€ì—­ ë¶„ì„ ì‹¤í–‰\"\"\"\n",
    "    \n",
    "    analyzer = DetailedRegionAnalyzer(client_id, client_secret)\n",
    "    results = analyzer.compare_top3_regions()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ê°œë³„ ì§€ì—­ ë¶„ì„ í•¨ìˆ˜\n",
    "def analyze_single_region(client_id, client_secret, region):\n",
    "    \"\"\"íŠ¹ì • ì§€ì—­ë§Œ ë¶„ì„\"\"\"\n",
    "    \n",
    "    analyzer = DetailedRegionAnalyzer(client_id, client_secret)\n",
    "    result = analyzer.comprehensive_region_analysis(region)\n",
    "    \n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸ” TOP 3 ì§€ì—­ ì‹¬ì¸µ ë¶„ì„ ë„êµ¬\")\n",
    "    print(\"í™ëŒ€, ì ì‹¤, ì¢…ë¡œë¥¼ ìƒì„¸ ë¹„êµí•©ë‹ˆë‹¤.\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"ğŸ’¡ ì‚¬ìš©ë²•:\")\n",
    "    print(\"analyzer = DetailedRegionAnalyzer('your_id', 'your_secret')\")\n",
    "    print(\"results = analyzer.compare_top3_regions()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ì¡´ API í‚¤ë¡œ TOP 3 ì§€ì—­ ì‹¬ì¸µ ë¶„ì„ ì‹¤í–‰\n",
    "analyzer = DetailedRegionAnalyzer(\"QjzwUZBwBgZijbaadyKV\", \"fIqgUfyng8\")\n",
    "results = analyzer.compare_top3_regions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class DetailedRegionAnalyzer:\n",
    "    def __init__(self, client_id, client_secret):\n",
    "        self.client_id = client_id\n",
    "        self.client_secret = client_secret\n",
    "        self.headers = {\n",
    "            'X-Naver-Client-Id': client_id,\n",
    "            'X-Naver-Client-Secret': client_secret\n",
    "        }\n",
    "        \n",
    "    def search_api(self, search_type, query, display=50):\n",
    "        url = f\"https://openapi.naver.com/v1/search/{search_type}.json\"\n",
    "        params = {'query': query, 'display': display, 'sort': 'date'}\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, params=params)\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            else:\n",
    "                return None\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "# ì‹¤í–‰\n",
    "analyzer = DetailedRegionAnalyzer(\"QjzwUZBwBgZijbaadyKV\", \"fIqgUfyng8\")\n",
    "\n",
    "print(\"ğŸ” í™ëŒ€, ì ì‹¤, ì¢…ë¡œ TOP 3 ì§€ì—­ ì‹¬ì¸µ ë¶„ì„ ì‹œì‘!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "regions = [\"í™ëŒ€\", \"ì ì‹¤\", \"ì¢…ë¡œ\"]\n",
    "detailed_results = {}\n",
    "\n",
    "for region in regions:\n",
    "    print(f\"\\nğŸ“ {region} ìƒì„¸ ë¶„ì„ ì¤‘...\")\n",
    "    \n",
    "    # 1. ê³ ê° íŠ¹ì„± ë¶„ì„\n",
    "    print(f\"ğŸ¯ {region} ê³ ê° íŠ¹ì„±:\")\n",
    "    customer_keywords = {\n",
    "        \"ëŒ€í•™ìƒ\": f\"{region} ê³±ì°½ ëŒ€í•™ìƒ\",\n",
    "        \"ì§ì¥ì¸\": f\"{region} ê³±ì°½ íšŒì‹\", \n",
    "        \"ê°€ì¡±\": f\"{region} ê³±ì°½ ê°€ì¡±\",\n",
    "        \"ì»¤í”Œ\": f\"{region} ê³±ì°½ ë°ì´íŠ¸\",\n",
    "        \"ê´€ê´‘ê°\": f\"{region} ê³±ì°½ ê´€ê´‘\"\n",
    "    }\n",
    "    \n",
    "    customer_data = {}\n",
    "    for customer_type, keyword in customer_keywords.items():\n",
    "        data = analyzer.search_api('blog', keyword, display=30)\n",
    "        mentions = data['total'] if data else 0\n",
    "        customer_data[customer_type] = mentions\n",
    "        if mentions > 0:\n",
    "            print(f\"   ğŸ‘¥ {customer_type}: {mentions:,}ê±´\")\n",
    "        time.sleep(0.3)\n",
    "    \n",
    "    # 2. ê²½ìŸ í™˜ê²½ ë¶„ì„\n",
    "    print(f\"\\nğŸª {region} ê²½ìŸ í™˜ê²½:\")\n",
    "    local_data = analyzer.search_api('local', f\"{region} ê³±ì°½ì§‘\", display=10)\n",
    "    restaurant_count = len(local_data['items']) if local_data and 'items' in local_data else 0\n",
    "    print(f\"   ğŸª ë“±ë¡ëœ ê³±ì°½ì§‘: {restaurant_count}ê°œ\")\n",
    "    \n",
    "    # 3. ê°€ê²© ë¯¼ê°ë„\n",
    "    print(f\"\\nğŸ’° {region} ê°€ê²© ë°˜ì‘:\")\n",
    "    price_keywords = {\n",
    "        \"ì €ë ´\": f\"{region} ê³±ì°½ ì €ë ´\",\n",
    "        \"ê°€ì„±ë¹„\": f\"{region} ê³±ì°½ ê°€ì„±ë¹„\",\n",
    "        \"ë¹„ì‹¸ë‹¤\": f\"{region} ê³±ì°½ ë¹„ì‹¸ë‹¤\"\n",
    "    }\n",
    "    \n",
    "    price_data = {}\n",
    "    for price_type, keyword in price_keywords.items():\n",
    "        data = analyzer.search_api('blog', keyword, display=20)\n",
    "        mentions = data['total'] if data else 0\n",
    "        price_data[price_type] = mentions\n",
    "        if mentions > 0:\n",
    "            print(f\"   ğŸ’µ {price_type}: {mentions:,}ê±´\")\n",
    "        time.sleep(0.3)\n",
    "    \n",
    "    # 4. ì§€ì—­ë³„ íŠ¹í™” ê¸°íšŒ\n",
    "    print(f\"\\nğŸ” {region} íŠ¹í™” ê¸°íšŒ:\")\n",
    "    if \"í™ëŒ€\" in region:\n",
    "        special_keywords = [f\"{region} ê³±ì°½ í™ìŠ¤í„°\", f\"{region} ê³±ì°½ í´ëŸ½\", f\"{region} ê³±ì°½ ì Šì€\"]\n",
    "    elif \"ì ì‹¤\" in region:\n",
    "        special_keywords = [f\"{region} ê³±ì°½ ê°€ì¡±\", f\"{region} ê³±ì°½ ë¡¯ë°ì›”ë“œ\", f\"{region} ê³±ì°½ ì£¼ë§\"]\n",
    "    elif \"ì¢…ë¡œ\" in region:\n",
    "        special_keywords = [f\"{region} ê³±ì°½ ì „í†µ\", f\"{region} ê³±ì°½ ê´€ê´‘\", f\"{region} ê³±ì°½ ì™¸êµ­ì¸\"]\n",
    "    \n",
    "    opportunities = {}\n",
    "    for keyword in special_keywords:\n",
    "        data = analyzer.search_api('blog', keyword, display=20)\n",
    "        mentions = data['total'] if data else 0\n",
    "        if mentions > 0:\n",
    "            clean_keyword = keyword.replace(f\"{region} ê³±ì°½ \", \"\")\n",
    "            opportunities[clean_keyword] = mentions\n",
    "            print(f\"   ğŸ’¡ {clean_keyword}: {mentions:,}ê±´\")\n",
    "        time.sleep(0.3)\n",
    "    \n",
    "    # 5. ì‹œê°„ëŒ€ë³„ ìˆ˜ìš”\n",
    "    print(f\"\\nâ° {region} ì‹œê°„ëŒ€ë³„ ìˆ˜ìš”:\")\n",
    "    time_keywords = [\"ì ì‹¬\", \"ì €ë…\", \"ì•¼ì‹\", \"ì£¼ë§\"]\n",
    "    time_data = {}\n",
    "    \n",
    "    for time_type in time_keywords:\n",
    "        data = analyzer.search_api('blog', f\"{region} ê³±ì°½ {time_type}\", display=20)\n",
    "        mentions = data['total'] if data else 0\n",
    "        time_data[time_type] = mentions\n",
    "        if mentions > 0:\n",
    "            print(f\"   ğŸ• {time_type}: {mentions:,}ê±´\")\n",
    "        time.sleep(0.3)\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    detailed_results[region] = {\n",
    "        'customer_data': customer_data,\n",
    "        'restaurant_count': restaurant_count,\n",
    "        'price_data': price_data,\n",
    "        'opportunities': opportunities,\n",
    "        'time_data': time_data\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… {region} ë¶„ì„ ì™„ë£Œ!\\n\")\n",
    "\n",
    "# ìµœì¢… ë¹„êµ ë¶„ì„\n",
    "print(\"ğŸ“Š TOP 3 ì§€ì—­ ì¢…í•© ë¹„êµ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for region, data in detailed_results.items():\n",
    "    print(f\"\\nğŸ† {region} ì¢…í•© í‰ê°€:\")\n",
    "    \n",
    "    # ì£¼ìš” ê³ ê°ì¸µ\n",
    "    main_customer = max(data['customer_data'].items(), key=lambda x: x[1])\n",
    "    if main_customer[1] > 0:\n",
    "        print(f\"   ğŸ¯ ì£¼ìš” ê³ ê°: {main_customer[0]} ({main_customer[1]:,}ê±´)\")\n",
    "    \n",
    "    # ê²½ìŸ ê°•ë„\n",
    "    competition_level = \"ë†’ìŒ\" if data['restaurant_count'] >= 10 else \"ì¤‘ê°„\" if data['restaurant_count'] >= 5 else \"ë‚®ìŒ\"\n",
    "    print(f\"   ğŸª ê²½ìŸ ê°•ë„: {competition_level} ({data['restaurant_count']}ê°œ)\")\n",
    "    \n",
    "    # ê°€ê²© íŠ¹ì„±\n",
    "    main_price = max(data['price_data'].items(), key=lambda x: x[1])\n",
    "    if main_price[1] > 0:\n",
    "        print(f\"   ğŸ’° ê°€ê²© íŠ¹ì„±: {main_price[0]} ì¤‘ì‹œ ({main_price[1]:,}ê±´)\")\n",
    "    \n",
    "    # íŠ¹í™” ê¸°íšŒ\n",
    "    if data['opportunities']:\n",
    "        top_opportunity = max(data['opportunities'].items(), key=lambda x: x[1])\n",
    "        print(f\"   ğŸ’¡ íŠ¹í™” ê¸°íšŒ: {top_opportunity[0]} ({top_opportunity[1]:,}ê±´)\")\n",
    "    \n",
    "    # ìµœì  ì‹œê°„\n",
    "    peak_time = max(data['time_data'].items(), key=lambda x: x[1])\n",
    "    if peak_time[1] > 0:\n",
    "        print(f\"   â° í”¼í¬ ì‹œê°„: {peak_time[0]} ({peak_time[1]:,}ê±´)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
